[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Welcome to my projects webpage. You will find here a list of ongoing or past projects I worked on."
  },
  {
    "objectID": "files/Ejercicios5Quarto.html",
    "href": "files/Ejercicios5Quarto.html",
    "title": "Ejercicios5",
    "section": "",
    "text": "El modelo No Restringido (NR) será: \\[ y_i = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_K x_{iK} + u_i \\]\nLos grados de libertad del denominador son: \\(gl_{den} = n - (K + 1)\\).\nEl modelo Restringido (R), bajo \\(H_0\\), es: \\[ y_i = \\beta_0 + u_i \\]\nEn este caso, la estimación MCO de \\(\\beta_0\\) es \\(\\bar{y}\\) (la media de y).\nLa Suma de Cuadrados de los Residuos del modelo restringido (\\(SRC_R\\)) es: \\[ SRC_R = \\sum (y_i - \\bar{y})^2 \\] Por definición, la suma de cuadrados de las desviaciones de ‘y’ respecto a su media es la Suma de Cuadrados Totales (SCT). Por tanto: \\[ SRC_R = SCT \\]\nEl número de restricciones (\\(q\\)) es igual al número de coeficientes que se igualan a cero en \\(H_0\\), que son todas las pendientes: \\[ q = K \\]\nSustituimos \\(SRC_R = SCT\\) y \\(q = K\\) en la fórmula general del estadístico F: \\[ F = \\frac{(SCT - SRC) / K}{SRC / (n - (K + 1))} \\quad \\text{(Expresión 1)} \\]\nEl coeficiente de determinación (\\(R^2\\)) se puede escribir como: \\[ R^2 = 1 - \\frac{SRC}{SCT} = \\frac{SCT - SRC}{SCT} \\]\nDespejamos \\(SRC\\): \\[ SRC = SCT - (R^2 \\cdot SCT) = SCT \\cdot (1 - R^2) \\]\nSustituimos las relaciones en la primera ecuación: \\[ F = \\frac{(R^2 \\cdot SCT) / K}{(SCT \\cdot (1 - R^2)) / (n - (K + 1))} \\]\nCancelamos SCT del numerador y del denominador: \\[ F = \\frac{R^2 / K}{(1 - R^2) / (n - (K + 1))} \\]\n\n\n\n\n\n\n# Parámetros\nset.seed(1234) \n\n# Generación variables\nx1 &lt;- runif(20, min = 0, max = 50)\nv  &lt;- rnorm(20, mean = 0, sd = 1)\nx2 &lt;- x1 + v # variable colineal\nx3 &lt;- runif(20, min = 0, max = 50)\nu &lt;- rnorm(20, mean = 0, sd = 8)\ny &lt;- 2 + 0.5 * x1 + 0.5 * x2 + 1 * x3 + u\n\n\n\n\n\nmodelo2b &lt;- lm(y ~ x1 + x2 + x3)\nsummary(modelo2b)\n\n\n\n\n\n# FIVs manuales\n\n# 1. FIV_1\naux_x1 &lt;- lm(x1 ~ x2 + x3)\nR2_x1 &lt;- summary(aux_x1)$r.squared\nFIV_1 &lt;- 1 / (1 - R2_x1)\n\n# 2. FIV_2\naux_x2 &lt;- lm(x2 ~ x1 + x3)\nR2_x2 &lt;- summary(aux_x2)$r.squared\nFIV_2 &lt;- 1 / (1 - R2_x2)\n\n# 3. FIV_3\naux_x3 &lt;- lm(x3 ~ x1 + x2)\nR2_x3 &lt;- summary(aux_x3)$r.squared\nFIV_3 &lt;- 1 / (1 - R2_x3)\n\nprint(c(FIV_1 = FIV_1, FIV_2 = FIV_2, FIV_3 = FIV_3))\n\n# FIVs automáticos\nlibrary(car)\nvif(modelo2b)\n\n\n\n\n\n# IC manuales\ncoef &lt;- as.numeric(modelo2b$coefficients)\nt_crit &lt;- qt(1 - 0.025, df = df.residual(modelo2b))\nes &lt;- as.numeric(summary(modelo2b)$coefficients[, \"Std. Error\"])\n\nIC_inf &lt;- coef - t_crit * es\nIC_sup &lt;- coef + t_crit * es\n\nIC_manual &lt;- cbind(IC_inf, IC_sup) \nprint(IC_manual)\n\n# IC automáticos\nconfint(modelo2b, level = 0.95)\n\n\n\n\n\n# Manual\nt_estadistico_manual &lt;- (coef - 0) / es\np_value_manual &lt;- 2 * (1 - pt(abs(t_estadistico_manual), df = df.residual(modelo2b)))\nsig_manual &lt;- cbind(t_estadistico_manual, p_value_manual)\nprint(sig_manual)\n\n# Automatizado\nsummary(modelo2b)$coefficients[, c(\"t value\", \"Pr(&gt;|t|)\")]\n\n\n\n\nContrastamos si \\(x_1\\) y \\(x_2\\) son conjuntamente significativos.\n\n# Modelo Restringido (solo x3)\nmodelo2f &lt;- lm(y ~ x3)\nSRC_R &lt;- sum(resid(modelo2f)^2) \nSRC_U &lt;- sum(resid(modelo2b)^2) # SCR del modelo completo\n\n# Estadístico F\nF_est &lt;- ((SRC_R - SRC_U) / 2) / (SRC_U / df.residual(modelo2b))\np_value_f &lt;- 1 - pf(F_est, df1 = 2, df2 = df.residual(modelo2b))\nprint(p_value_f)\n\n# Automatizado\nhypothesis &lt;- c(\"x1 = 0\", \"x2 = 0\")\ntest &lt;- linearHypothesis(modelo2b, hypothesis)\nprint(test)\n\n\n\n\nLa fuerte colinealidad introducida entre \\(x_1\\) y \\(x_2\\) (FIV alto) ha impactado negativamente en la inferencia estadística:\n\nInflación de la varianza: Los errores estándar de los coeficientes de \\(x_1\\) y \\(x_2\\) se han inflado significativamente.\nIntervalos de confianza anchos: La varianza inflada resulta en intervalos muy amplios.\nDificultad para rechazar \\(H_0\\) individual: El estadístico t se reduce, dificultando la detección de un efecto real.\nParadoja de la colinealidad: El test F conjunto logra rechazar \\(H_0\\), demostrando que el conjunto de variables es importante aunque individualmente no lo parezcan.\n\n\n\n\n\n# Parámetros\nset.seed(1234) \n# Generación variables (n=200)\nx1 &lt;- runif(200, min = 0, max = 50)\nv  &lt;- rnorm(200, mean = 0, sd = 1)\nx2 &lt;- x1 + v \nx3 &lt;- runif(200, min = 0, max = 50)\nu &lt;- rnorm(200, mean = 0, sd = 8)\ny &lt;- 2 + 0.5 * x1 + 0.5 * x2 + 1 * x3 + u\n\nmodelo2h &lt;- lm(y ~ x1 + x2 + x3)\nsummary(modelo2h)\n\n# FIVs para n=200\nvif(modelo2h)\n\n# Test F conjunto\nmodelo2hh &lt;- lm(y ~ x3)\nSRC_R &lt;- sum(resid(modelo2hh)^2) \nSRC_U &lt;- sum(resid(modelo2h)^2) \nF_est &lt;- ((SRC_R - SRC_U) / 2) / (SRC_U / df.residual(modelo2h))\nprint(F_est)\n\n\nLa colinealidad persiste: Los valores FIV para \\(x_1\\) y \\(x_2\\) siguen siendo extremadamente altos (~210), similares a los de \\(n=20\\). La colinealidad es estructural.\nLos estimadores han cambiado: Los errores estándar se han reducido considerablemente. Sin embargo, a pesar de aumentar \\(n\\) a 200, los p-values pueden seguir siendo altos si la colinealidad es extrema.\nLa expresión clave: \\[ Var(\\hat{\\beta}_j) = \\frac{\\sigma^2}{SCT_j (1-R^2_j)} = \\frac{\\sigma^2}{SCT_j} \\cdot FIV_j \\] El tamaño de la muestra afecta a la \\(SCT_j\\) (Suma de Cuadrados Totales de x). Cuanto mayor es \\(n\\), mayor es \\(SCT_j\\), lo que ayuda a reducir la varianza y contrarrestar el efecto del FIV alto.\n\n\n\n\n\n\neducation &lt;- read.csv(\"education.csv\")\n\n\n\n\n\\(\\beta_1\\) (sibs): NEGATIVO (-). En familias más grandes, hay menos recursos per cápita.\n\\(\\beta_2\\) (meduc): POSITIVO (+). Madres más educadas valoran más la educación.\n\\(\\beta_3\\) (feduc): POSITIVO (+). Padres más educados proveen mejor entorno académico.\n\n\n\n\n\nmodelo3b &lt;- lm(educ ~ sibs + meduc + feduc, data = education)\nsummary(modelo3b)\nlibrary(stargazer)\nstargazer(modelo3b, type = \"text\", \n          dep.var.labels = \"educación (en años)\", \n          covariate.labels = c(\"n. hermanos/as\", \"educación madre\", \"educación padre\", \"Intercept\"))\n\n\n\n\n\ncoefs &lt;- coef(modelo3b)\nerrores &lt;- summary(modelo3b)$coefficients[, \"Std. Error\"]\nr2 &lt;- summary(modelo3b)$r.squared\n\ncat(sprintf(\"educ = %.4f (%.4f) %.4f (%.4f) * sibs + %.4f (%.4f) * meduc + %.4f (%.4f) * feduc\\n\",\n            coefs[1], errores[1], coefs[2], errores[2], coefs[3], errores[3], coefs[4], errores[4]))\ncat(sprintf(\"R^2 = %.4f\\n\", r2))\n\n\\[educ = 10.3643 (0.3585) -0.0936 (0.0345) * sibs + 0.1308 (0.0327) * meduc + 0.2100 (0.0275) * feduc\\]\n\n\n\n\nt_b1 &lt;- abs(coefs[2] / errores[2])\nt_crit &lt;- qt(1 - 0.01/2, df = df.residual(modelo3b)) \nprint(t_b1)\nprint(t_crit)\n\nConclusión: Rechazamos \\(H_0\\).\n\n\n\n\n# F = t^2 para una restricción\nF_b1 &lt;- t_b1^2\nF_crit &lt;- qf(1 - 0.01, df1 = 1, df2 = df.residual(modelo3b))\nprint(F_b1)\nprint(F_crit)\n\nConclusión: Rechazamos \\(H_0\\).\n\n\n\n\nt_b2 &lt;- abs(coefs[3] / errores[3])\np_value_b2 &lt;- 2 * (1 - pt(t_b2, df = df.residual(modelo3b)))\nprint(p_value_b2)\n\n\n\n\n\nt_b3 &lt;- abs(coefs[4] / errores[4])\nlim &lt;- max(4, abs(t_b3) + 1)\ncurve(dt(x, df.residual(modelo3b)), -lim, lim, lwd=2, ylab=\"Densidad\", \n      main=paste(\"Estadístico t para feduc (t =\", round(t_b3, 4), \")\"))\nabline(v = c(t_b3, -t_b3), col=\"blue\", lwd=2, lty=2)\n\n\n\n\n\n*** \\(p &lt; 0.001\\) (Significativo al 0.1%)\n** \\(p &lt; 0.01\\) (Significativo al 1%)\n* \\(p &lt; 0.05\\) (Significativo al 5%)\n\n\n\n\n\nbeta1 &lt;- coefs[2]\nt_crit_95 &lt;- qt(1 - 0.025, df.residual(modelo3b))\nes_sibs &lt;- errores[2]\n\nIC_inf &lt;- beta1 - t_crit_95 * es_sibs\nIC_sup &lt;- beta1 + t_crit_95 * es_sibs\nprint(c(IC_inf, IC_sup))\n\n# Automático\nconfint(modelo3b, level = 0.95)[2,]\n\n\n\n\nEs probable que haya colinealidad porque las personas tienden a casarse con parejas de nivel educativo similar, lo que genera alta correlación entre meduc y feduc.\n\n\n\n\nvif(modelo3b)\n\n\n\n\n\nmodelo3l &lt;- lm(educ ~ 1, data=education) # Solo constante\n\nSRC_R &lt;- sum(resid(modelo3l)^2) # Es la SCT (Suma Cuadrados Total)\nSRC_U &lt;- sum(resid(modelo3b)^2)\n\nF_global &lt;- ((SRC_R - SRC_U) / 3) / (SRC_U / df.residual(modelo3b))\np_value_global &lt;- 1 - pf(F_global, df1=3, df2=df.residual(modelo3b))\nprint(F_global)\n\n# comprobamos con el summary\nprint(summary(modelo3b)$fstatistic)\n\n\n\n\n\nF_global_r2 &lt;- (r2 / 3) / ((1 - r2) / df.residual(modelo3b))\nprint(F_global_r2)\n\n\n\n\nQueremos saber si el efecto de la educación de la madre (beta2) es igual a lo del padre (beta3) \\[H_0: \\beta_2 - \\beta_3 = 0\\]\nSi queremos escribirlo como modelo restringido \\[ y = \\beta_0 + \\beta_1sibs + \\beta_2(meduc + feduc) + u\\]\n\n\n\nContrastamos si el efecto de la educación de la madre es igual al del padre.\n\n# Variable auxiliar suma\neducation$padres_sum &lt;- education$meduc + education$feduc\n# Modelo restringido: educ = b0 + b1*sibs + b2(padres_sum)\nmodelo3o &lt;- lm(educ ~ sibs + padres_sum, data=education)\n\nSRC_R_padres &lt;- sum(resid(modelo3o)^2)\nF_padres &lt;- ((SRC_R_padres - SRC_U) / 1) / (SRC_U / df.residual(modelo3b))\np_val_padres &lt;- 1 - pf(F_padres, df1=1, df2=df.residual(modelo3b))\n\nprint(F_padres)\nprint(p_val_padres)\n\nConclusión: El p-value es alto, no podemos rechazar \\(H_0\\). No hay evidencia de que los efectos sean distintos.\n\n\n\n\n\nlibrary(readr)\nvote1 &lt;- read.csv(\"vote1.csv\")\n\n\n\nExpectativas: \\(\\beta_1 &gt; 0\\), \\(\\beta_2 &lt; 0\\), \\(\\beta_3 &gt; 0\\)\n\nlibrary(stargazer)\nmodelo4a&lt;- lm(votA ~ expendA + expendB + prtystrA, data = vote1)\nstargazer(modelo4a, type = \"text\")\n\n\n\n\nGasto propio no afecta: \\[H_0: \\beta_1 = 0\\] Gasto rival no afecta: \\[H_0: \\beta_2 = 0\\]\nSi miramos los dos t estimados en summary(modelo4a), ya podemos rechazar las dos H_0\n\nsummary(modelo4a)\n\n\n\n\n\\[H_0: \\beta_1=-\\beta_2\\] Con esta hipotesis nula contrastamos si el coefficiente del gasto del candidato A tiene el efecto opuesto al gasto del candidato B\n\nhypothesis &lt;- c(\"expendA + expendB = 0\")\ntest &lt;- linearHypothesis (modelo4a, hypothesis)\nprint(test)\n\n# De manera manual\n# Error estándar de la suma: sqrt(Var(b1) + Var(b2) + 2*Cov(b1,b2))\nse_suma &lt;- sqrt(vcov(modelo4a)[\"expendA\",\"expendA\"] + \n                  vcov(modelo4a)[\"expendB\",\"expendB\"] + \n                  2*vcov(modelo4a)[\"expendA\",\"expendB\"])\n\nt_stat_manual &lt;- (modelo4a$coefficients[\"expendA\"] + modelo4a$coefficients[\"expendB\"]) / se_suma\np_val_manual &lt;- 2 * (1 - pt(abs(t_stat_manual), df.residual(modelo4a)))\n\nprint(t_stat_manual)\nprint(p_val_manual)\n\nNo podemos rechazar \\(H_0\\). Hay un problema de colinealidad!\n\n\n\n\nSRC_U &lt;- sum(resid(modelo4a)^2)\n# Creamos el modelo restringido: votA = beta0 + beta1(expendA+expendB)+beta2*prtystrA\nexpendAB &lt;-  vote1$expendA - vote1$expendB #creación variable combinada\nmodelo4d &lt;- lm(votA ~ expendAB + prtystrA, data = vote1)\nSRC_R &lt;- sum(resid(modelo4d)^2)\n\nF_est &lt;- ((SRC_R - SRC_U) / 1) / (SRC_U / df.residual(modelo4a))\nF_crit_d &lt;- qf(1 - 0.05, df1 = 1, df2 = df.residual(modelo4a)) # F crítico\np_value &lt;- 1 - pf(F_est, df1 = 1, df2 = df.residual(modelo4a))\nprint(p_value) # el mismo que con el test t\n\n\n\n\n\n\n\nMide el efecto marginal de la penetración de redes sociales sobre la corrupción, ceteris paribus (manteniendo constante la renta per cápita).\n\n\n\n\nmodelo5b &lt;- lm(corruption ~ socialmediashare + gdppc, data = corruption)\nlibrary(stargazer)\nstargazer(modelo5b, type = \"text\")\n\nEl modelo explica el 74.8% de la variabilidad de la corrupción\n\\(\\beta_0\\): si un país tuviera 0 penetración de redes sociales y 0 renta per cápita, su índice de corrupción predicho sería de 100.6 puntos.\n\\(\\beta_1\\): manteniendo constante la renta per cápita, un aumento de 1 punto en la penetración de redes sociales está asociado a una disminución media de 0.0015 puntos en el índice de corrupción\n\\(\\beta_2\\): manteniendo constante la penetración de redes sociales, un aumento de 1 unidad en la variable de riqueza está asociado a una disminución media de 0.3803 puntos en el índice de corrupción\n\n\n\nExpectativa teórica\nEs razonable esperar que los países con mayor renta per cápita (más ricos) tengan mejores infraestructuras de telecomunicaciones, mayor acceso a tecnología (smartphones, ordenadores), y mayor alfabetización digital, lo que facilita una mayor penetración de las redes sociales\n\ncorrb1b2 &lt;- cor(corruption$socialmediashare, corruption$gdppc)\nprint(corrb1b2)\n\nConclusión: la correlación es positiva y muy alta\n\n\n\nSabemos que la riqueza (gdppc) reduce la corrupción (coeficiente negativo: -0.38) Hemos establecido en 5.c que la riqueza está correlacionada positivamente con las redes sociales. Por lo tanto, esperamos un sesgo de variable omitida negativo\nSi omitimos gdppc, el coeficiente de socialmediashare sufriría un sesgo negativo volviendose más negativo (en valor absoluto) de lo que realmente es. Comprobamolo:\n\nmodelo5d &lt;- lm(corruption ~ socialmediashare, data = corruption)\nsummary(modelo5d)\n\nPodemos confirmar nuestro asunto sobre la variable omitida\n\n\n\n\n\nrm(list = ls()) # limpiamos\nlibrary(readr)\nairfares &lt;- read.csv(\"airfares.csv\")\n\n\n\n\nmodelo6a &lt;- lm(log(fare) ~ log(disthm) + concen, data = airfares)\nsummary(modelo6a)\nlibrary(stargazer)\nstargazer(modelo6a, type = \"text\", dep.var.labels = \"Precio vuelos\", \n          covariate.labels = c(\"Distancia en 100 millas (log)\", \"Cuota de mercado (log)\", \"Intercepta\"))\n\n\n\n\nAl ser un modelo log-log, \\(\\beta_1\\) representa la elasticidad precio-distancia. Si la distancia aumenta un \\(1\\%\\), se estima que el precio del billete aumentará, en promedio, un \\(0.4\\%\\), manteniendo constante la concentración de mercado\n\n\n\n\nlibrary(car)\n# t = (beta1 - 0.5) / se(beta1)\nhypothesis &lt;- c(\"log(disthm) = 0.5\")\ntest &lt;- linearHypothesis(modelo6a, hypothesis)\nprint(test)\n# Nota: linearHypothesis usa el estadístico F. Recuerda que F = t^2 para 1 restricción\n\n# manual\nt_est &lt;- (summary(modelo6a)$coefficients[\"log(disthm)\",\"Estimate\"] - 0.5) / summary(modelo6a)$coefficients[\"log(disthm)\", \"Std. Error\"]\nprint(t_est)\nt_crit &lt;- qt(1 - 0.5/2, df = df.residual(modelo6a))\nprint(t_crit)\n\nRechazamos \\(H_0\\)\n\n\n\n\\(H_0: \\beta_1=0.5\\) contrasta si la elasticidad precio-distancia es exactamente \\(0.5\\) Es decir, si ante un aumento del \\(1\\%\\) en la distancia, el precio sube exactamente un \\(0.5\\%\\)\n\n\n\n\ncoef &lt;- as.numeric(modelo6a$coefficients[2])\nt_crit &lt;- qt(1 - 0.025, df = df.residual(modelo6a))\nes &lt;- as.numeric(summary(modelo6a)$coefficients[2, \"Std. Error\"])\n\n\\[CI=0.408621752 \\pm 1.962036 * 0.01776081\\]\n\nIC_inf &lt;- coef - t_crit * es\nIC_sup &lt;- coef + t_crit * es\n\nIC_manual &lt;- cbind(IC_inf, IC_sup) \nprint(IC_manual)\n\n# IC automáticos\nIC_95 &lt;- confint(modelo6a, level = 0.95)\nprint(IC_95)\n\nDado los valores del límite inferior y superior, podemos decir, con una confianza del \\(95\\%\\), que si la distancia augmenta en un \\(1\\%\\) el precio aumentará entre un \\(0.373774407\\) y un \\(0.443469097\\)\n\n\n\nSignifica que si tomáramos muchas muestras aleatorias diferentes de vuelos y construyéramos un intervalo de confianza para cada una el \\(95\\%\\) de esos intervalos contendrían el verdadero valor poblacional del parámetro \\(\\beta_1\\)\n\n\n\nPodemos mirar a la \\(t\\) sacada por el summary\n\nsummary(modelo6a)$coefficients[\"log(disthm)\", \"t value\"]\n\nCalculado manualmente como \\(\\hat{\\beta}_1 / se(\\hat{\\beta}_1)\\)\n\nt_est &lt;- (summary(modelo6a)$coefficients[\"log(disthm)\",\"Estimate\"] - 0) / summary(modelo6a)$coefficients[\"log(disthm)\", \"Std. Error\"]\nprint(t_est)\n\nt_crit &lt;- qt(1 - 0.5/2, df = df.residual(modelo6a))\nprint(t_crit)\n\n\\[|t_{est}| &gt; |t_{crit}| \\] Rechazamos \\(H_0\\)\n\np_val_manual &lt;- 2 * (1 - pt(abs(t_est), df.residual(modelo6a)))\nprint(p_val_manual)\n\n\n\n\n\nSRC_U &lt;- sum(resid(modelo6a)^2)\n\n# Creamos el modelo restringido: log(fare) = concen\nmodelo6h &lt;- lm(log(fare) ~ concen, data = airfares)\nSRC_R &lt;- sum(resid(modelo6h)^2)\n\nF_est &lt;- ((SRC_R - SRC_U) / 1) / (SRC_U / df.residual(modelo6a)) # exactamente t^2\nF_crit_d &lt;- qf(1 - 0.05, df1 = 1, df2 = df.residual(modelo6a)) # F crítico\np_value &lt;- 1 - pf(F_est, df1 = 1, df2 = df.residual(modelo6a))\nprint(p_value) # el mismo que con el test t\n\n\n\n\n\nmodelo6i &lt;- lm(log(fare) ~ log(disthm*100) + concen, data = airfares)\nsummary(modelo6i)\n\nMatematicamente, podemos dividir este logaritmo como ln(100) + ln(disthm) adonde el primer término se convierte ser una constante. Esta constante se suma a la constante De hecho, la intercepta será lo único que cambia con respeto al primer modelo.\n\n\n\n\n\nrm(list = ls()) # limpiamos\nlibrary(readr)\nhprice2 &lt;- read.csv(\"hprice2.csv\")\n\n\n\n\nmodelo7a &lt;- lm(log(price) ~ rooms + log(nox) + log(dist) + stratio + crime, data = hprice2)\nsummary(modelo7a)\n\n\n\n\n\n\\(\\beta_1\\): positivo Más habitaciones implican una casa más grande, aumentando el precio\n\\(\\beta_2\\): negativo Más contaminación hace la zona menos deseable, bajando el precio\n\\(\\beta_3\\): negativo Estar lejos del centro reduce el valor. A veces puede ser positivo si estar lejos implica estar en suburbios limpios y tranquilos\n\\(\\beta_4\\): negativo Un ratio alto indica clases masificadas (= peor calidad escolar), reduciendo el valor de la zona\n\\(\\beta_5\\): negativo Más crimen hace la zona peligrosa y menos deseable\n\n\n\n\nTodos los estimadores han salido significativos al \\(1\\%\\)\n\n\n\nManteniendo constantes el resto de características, si una vivienda tiene una habitación más que otra (\\(\\Delta rooms = 1\\)), se estima que su precio será aproximadamente un \\(24.6\\%\\) mayor\n\n\n\nManteniendo constantes el resto de variables, si el nivel de contaminación aumenta un \\(1\\%\\), se estima que el precio de la vivienda disminuirá un \\(0.9\\%\\)\n\n\n\nBajo el supuesto de Media Condicional Cero (MLR.2): \\(E(u | nox, rooms, ...) = 0\\) Esto exige que la variable \\(log(nox)\\) sea exógena, es decir, que no esté correlacionada con el término de error \\(u\\). Es muy probable que existan variables omitidas en \\(u\\) (como la calidadestética del barrio, ruido, antigüedad de edificios) que estén correlacionadas con la contaminación. Podemos concluir que haya un sesgo de variable omitida\n\n\n\n\n\nrm(list = ls()) # limpiamos\nlibrary(readr)\nmicrosoft &lt;- read.csv(\"microsoft.csv\")\n\n\n\n\\[\\beta_6 = \\mathbb{E}(RPmsoft | m1=1, X) - \\mathbb{E}(RPmsoft | m1=0, X)\\] Donde \\(X\\) representa el resto de variables constantes (Ceteris Paribus) \\(\\beta_6\\) capta la diferencia esperada en la prima de riesgo de Microsoft en el mes de Enero (\\(m1=1\\)) comparada con el resto de meses del año, manteniendo constantes las condiciones de mercado aproximadas por los indices S&P500 y otras variables macroeconómicas\n\n\n\n\nmodelo8b &lt;- lm(RPmsoft ~ RPsandp + Dprod + Dinflation + Dterm + m1, data = microsoft)\nsummary(modelo8b)\n\n\n\n\n\nt_beta6 &lt;- coef(modelo8b)[\"m1\"] / coef(summary(modelo8b))[\"m1\", \"Std. Error\"]\nt_crit &lt;- qt(1 - 0.5/2, df = df.residual(modelo8b))\nprint(t_beta6)\nprint(t_crit)\n\n\\(H_0: \\beta_6 = 0\\) =&gt; No hay efecto diferencial en enero \\(H1: \\beta_6 \\neq 0\\) =&gt; Existe estacionalidad en enero\nNo rechazamos \\(H_0\\) La variable \\(m1\\) NO es significativa. No hay evidencia suficiente de un efecto diferencial\n\n\n\n\\[p-value = P(|T| &gt; |t_{est}|) = 2 * (1 - P(T &lt;= |t_{est}|))\\]\n\np_val_manual &lt;- 2 * (1 - pt(abs(t_beta6), df.residual(modelo8b)))\nprint(p_val_manual)\n\nGráfico:\n\n\n\nEl Efecto Enero sostiene que los rendimientos en enero tienden a ser más altos Aunque el signo pueda ser positivo, el efecto no es estadísticamente significativo No hay evidencia estadística suficiente para apoyar el Efecto Enero\n\n\n\n\n\nlibrary(readr)\nrm(list = ls()) # limpiamos\ncps09 &lt;- read.csv(\"cps09subset.csv\")\n\n\n\n\nmodelo9a &lt;- lm(log(earnings) ~ ed + ex, data = cps09) # ecuación de Mincer!\nsummary(modelo9a)\n\n\n\n\nEn el Modelo 1, la relación entre el log del salario y la experiencia es lineal Si la experiencia aumenta de \\(1\\) a \\(2\\) años (\\(\\Delta ex = 1\\)), el salario esperado aumenta aproximadamente \\(1\\%\\), manteniendo la educación constante\n\n\n\nEn este Modelo 1, la variación esperada es exactamente la misma que en el apartado (b) El aumento estimado sigue siendo del \\(1\\%\\). El modelo lineal impone que el efecto marginal de la experiencia sea constante independientemente de cuánta experiencia se tenga acumulada\n\n\n\nEl término cuadratico sirve para capturar los rendimientos marginales decrecientes En la vida real, el salario suele subir rápido al inicio de la carrera (se aprende mucho) pero luego se estanca o crece más despacio (o incluso baja al final). Una línea recta no capta esto, pero una parábola (función cuadrática)\nEsperamos un signo positivo para \\(\\beta_2\\) y negativo para \\(\\beta_3\\). ¿Porqué? Para tener una forma de ‘U invertida’ (concava) que sube y luego baja (o se aplana), el coeficiente del la primera derivada (\\(\\beta_2\\)) tiene que ser positivo mientras el signo de la segunda derivada (\\(\\beta_3\\)) tiene que ser negativo\n\n\n\n\nmodelo9e &lt;- lm(log(earnings) ~ ed + ex + I(ex^2), data = cps09)\nsummary(modelo9e)\nlibrary(stargazer)\nstargazer(modelo9a, modelo9e, type = \"text\")\n\n\n\n\nEl signo de \\(\\beta_3\\) es negativo, tal como esperábamos Cómo cambian las interpretaciones de las preguntas (b) y (c)? Si la experiencia aumenta de \\(1\\) a \\(2\\) años (\\(\\Delta ex = 1\\)), el salario esperado aumenta \\[0.039*1+2*(-0.001)*1=0.037\\] Si la experiencia aumenta de \\(31\\) a \\(32\\) años (\\(\\Delta ex = 31\\)), el salario esperado aumenta \\[0.039*31+2*(-0.001)*31=1.147\\] ### 9.g) Significación \\(\\beta_3\\)\n\nt_beta3 &lt;- coef(modelo9e)[\"I(ex^2)\"] / coef(summary(modelo9e))[\"I(ex^2)\", \"Std. Error\"]\nt_crit &lt;- qt(1 - 0.01/2, df = df.residual(modelo9e)) # con alpha=0.1\nprint(t_beta3)\nprint(t_crit)\n\nEl regresor \\(ex^2\\) es significativo al \\(1\\%\\), confirmando la no linealidad\n\n\n\n\n\nrm(list = ls()) # limpiamos\nlibrary(readr)\ncoffee2m &lt;- read.csv(\"coffee2m.csv\")\n\n\n\nModelo completo \\[ln(Sales) = \\beta_0 + \\beta_1 * ln(price) + \\beta_2 * ad + \\beta_3(ad * log(price))\\]\nModelo con publicidad \\[\\mathbb{E}[sales|ad=1]= (\\beta_0 + \\beta_2) + (\\beta_2 + \\beta4) * log(price)\\]\nModelo sin publicidad \\[ \\mathbb{E} [sales|ad=0]= \\beta_0 + \\beta_1 * log(price) \\] ### 10.b) Parámetros elasticidad\nAquí miramos al modelo completo Elasticidad cuando hay publicidad \\[\\mathbb{E}[sales|price, ad=1] = \\beta_1 + \\beta_3 \\]\nElasticidad cuando no hay publicidad \\[\\mathbb{E}[sales|price, ad=0] = \\beta_1\\]\nSi \\(|\\beta_1 + \\beta_3| &gt; |\\beta_1|\\), la publicidad hace que la demanda sea más elásticay los consumidores se vuelven más sensibles al precio\nSi \\(|\\beta_1 + \\beta_3| &lt; |\\beta_1|\\), la publicidad hace que la demanda sea menos elásticay los consumidores se vuelven menos sensibles al precio\n\n\n\n\nmodelo10c &lt;- lm(log(sales) ~ log(price) * ad, data = coffee2m)\nlibrary(stargazer)\nstargazer(modelo10c, type = \"text\")\n\nCeteris paribus, las semanas con campaña publicitaria tienen en promedio, un volumen de ventas un \\(25.6\\%\\) mayor aproximadamente\nPero el cambio en la elasticidad no es estadísticamente significativo\n\n\n\nTenemos que contrastar ele efecto de la publicidad que afecta dos regresores: \\(\\beta_2\\) y \\(\\beta_3\\). Entonces nuestra hipotesis será \\[H_0: \\beta_2=0 \\quad \\beta_3=0\\]\nModelo restringido\n\nmodelo10d &lt;- lm(log(sales) ~ log(price), data = coffee2m)\n\nSRC_R &lt;- sum(resid(modelo10d)^2)\nSRC_U &lt;- sum(resid(modelo10c)^2)\n\nF_est &lt;- ((SRC_R - SRC_U) / 1) / (SRC_U / df.residual(modelo10c))\nF_crit_d &lt;- qf(1 - 0.01, df1 = 2, df2 = df.residual(modelo10c)) # F crítico con alpha=0.01\np_value &lt;- 1 - pf(F_est, df1 = 2, df2 = df.residual(modelo10c))\nprint(p_value) \n\nContraste \\(F\\) automatizado con linearHypothesis\n\nlibrary(car)\n\nhipotesis &lt;- c(\"ad = 0\", \"log(price):ad = 0\")\ntest_F_ad &lt;- linearHypothesis(modelo10c, hipotesis)\nprint(test_F_ad)\n\nEn este caso los comandos automatizados no nos permite establecer \\(\\alpha = 1\\%\\) Rechazamos \\(H_0\\)\n\n\n\n\nt_beta3 &lt;- coef(modelo10c)[\"log(price):ad\"] / coef(summary(modelo10c))[\"log(price):ad\", \"Std. Error\"]\nt_crit &lt;- qt(1 - 0.01/2, df = df.residual(modelo10c))\nprint(t_beta3)\nprint(t_crit)\n\nNo rechazamos \\(H_0\\)\n\n\n\n\nt_crit &lt;- qt(1 - 0.05/2, df = df.residual(modelo10c)) #t con alpha=0.05\n\nic_inf &lt;- coef(modelo10c)[\"log(price):ad\"] - t_crit * coef(summary(modelo10c))[\"log(price):ad\", \"Std. Error\"]\nic_sup &lt;- coef(modelo10c)[\"log(price):ad\"] + t_crit * coef(summary(modelo10c))[\"log(price):ad\", \"Std. Error\"]\nprint(c(ic_inf, ic_sup))\n\nIntervalos de confianza automatizado con confint\n\nIC_95 &lt;- confint(modelo10c, level = 0.95)\nprint(IC_95)\n\nEl intervalo de \\(\\beta_3\\) pasa por cero, entonces no podemos rechazar \\(H_0\\) ¿Cómo mejorar el experimento para medir \\(\\beta_3\\) con más precisión?\n\nAumentar el tamaño de la muestra (\\(n=16\\))\nAumentar la variabilidad de los precios (más días, más semanas = más precios observados)\nAumentar la submuestra de precios con publicidad (muestra más equilibrada)\nIncluir variables de control: temperatura, festivos, localización, etc."
  },
  {
    "objectID": "files/Ejercicios5Quarto.html#ejercicio-1",
    "href": "files/Ejercicios5Quarto.html#ejercicio-1",
    "title": "Ejercicios5",
    "section": "",
    "text": "El modelo No Restringido (NR) será: \\[ y_i = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_K x_{iK} + u_i \\]\nLos grados de libertad del denominador son: \\(gl_{den} = n - (K + 1)\\).\nEl modelo Restringido (R), bajo \\(H_0\\), es: \\[ y_i = \\beta_0 + u_i \\]\nEn este caso, la estimación MCO de \\(\\beta_0\\) es \\(\\bar{y}\\) (la media de y).\nLa Suma de Cuadrados de los Residuos del modelo restringido (\\(SRC_R\\)) es: \\[ SRC_R = \\sum (y_i - \\bar{y})^2 \\] Por definición, la suma de cuadrados de las desviaciones de ‘y’ respecto a su media es la Suma de Cuadrados Totales (SCT). Por tanto: \\[ SRC_R = SCT \\]\nEl número de restricciones (\\(q\\)) es igual al número de coeficientes que se igualan a cero en \\(H_0\\), que son todas las pendientes: \\[ q = K \\]\nSustituimos \\(SRC_R = SCT\\) y \\(q = K\\) en la fórmula general del estadístico F: \\[ F = \\frac{(SCT - SRC) / K}{SRC / (n - (K + 1))} \\quad \\text{(Expresión 1)} \\]\nEl coeficiente de determinación (\\(R^2\\)) se puede escribir como: \\[ R^2 = 1 - \\frac{SRC}{SCT} = \\frac{SCT - SRC}{SCT} \\]\nDespejamos \\(SRC\\): \\[ SRC = SCT - (R^2 \\cdot SCT) = SCT \\cdot (1 - R^2) \\]\nSustituimos las relaciones en la primera ecuación: \\[ F = \\frac{(R^2 \\cdot SCT) / K}{(SCT \\cdot (1 - R^2)) / (n - (K + 1))} \\]\nCancelamos SCT del numerador y del denominador: \\[ F = \\frac{R^2 / K}{(1 - R^2) / (n - (K + 1))} \\]"
  },
  {
    "objectID": "files/Ejercicios5Quarto.html#ejercicio-2",
    "href": "files/Ejercicios5Quarto.html#ejercicio-2",
    "title": "Ejercicios5",
    "section": "",
    "text": "# Parámetros\nset.seed(1234) \n\n# Generación variables\nx1 &lt;- runif(20, min = 0, max = 50)\nv  &lt;- rnorm(20, mean = 0, sd = 1)\nx2 &lt;- x1 + v # variable colineal\nx3 &lt;- runif(20, min = 0, max = 50)\nu &lt;- rnorm(20, mean = 0, sd = 8)\ny &lt;- 2 + 0.5 * x1 + 0.5 * x2 + 1 * x3 + u\n\n\n\n\n\nmodelo2b &lt;- lm(y ~ x1 + x2 + x3)\nsummary(modelo2b)\n\n\n\n\n\n# FIVs manuales\n\n# 1. FIV_1\naux_x1 &lt;- lm(x1 ~ x2 + x3)\nR2_x1 &lt;- summary(aux_x1)$r.squared\nFIV_1 &lt;- 1 / (1 - R2_x1)\n\n# 2. FIV_2\naux_x2 &lt;- lm(x2 ~ x1 + x3)\nR2_x2 &lt;- summary(aux_x2)$r.squared\nFIV_2 &lt;- 1 / (1 - R2_x2)\n\n# 3. FIV_3\naux_x3 &lt;- lm(x3 ~ x1 + x2)\nR2_x3 &lt;- summary(aux_x3)$r.squared\nFIV_3 &lt;- 1 / (1 - R2_x3)\n\nprint(c(FIV_1 = FIV_1, FIV_2 = FIV_2, FIV_3 = FIV_3))\n\n# FIVs automáticos\nlibrary(car)\nvif(modelo2b)\n\n\n\n\n\n# IC manuales\ncoef &lt;- as.numeric(modelo2b$coefficients)\nt_crit &lt;- qt(1 - 0.025, df = df.residual(modelo2b))\nes &lt;- as.numeric(summary(modelo2b)$coefficients[, \"Std. Error\"])\n\nIC_inf &lt;- coef - t_crit * es\nIC_sup &lt;- coef + t_crit * es\n\nIC_manual &lt;- cbind(IC_inf, IC_sup) \nprint(IC_manual)\n\n# IC automáticos\nconfint(modelo2b, level = 0.95)\n\n\n\n\n\n# Manual\nt_estadistico_manual &lt;- (coef - 0) / es\np_value_manual &lt;- 2 * (1 - pt(abs(t_estadistico_manual), df = df.residual(modelo2b)))\nsig_manual &lt;- cbind(t_estadistico_manual, p_value_manual)\nprint(sig_manual)\n\n# Automatizado\nsummary(modelo2b)$coefficients[, c(\"t value\", \"Pr(&gt;|t|)\")]\n\n\n\n\nContrastamos si \\(x_1\\) y \\(x_2\\) son conjuntamente significativos.\n\n# Modelo Restringido (solo x3)\nmodelo2f &lt;- lm(y ~ x3)\nSRC_R &lt;- sum(resid(modelo2f)^2) \nSRC_U &lt;- sum(resid(modelo2b)^2) # SCR del modelo completo\n\n# Estadístico F\nF_est &lt;- ((SRC_R - SRC_U) / 2) / (SRC_U / df.residual(modelo2b))\np_value_f &lt;- 1 - pf(F_est, df1 = 2, df2 = df.residual(modelo2b))\nprint(p_value_f)\n\n# Automatizado\nhypothesis &lt;- c(\"x1 = 0\", \"x2 = 0\")\ntest &lt;- linearHypothesis(modelo2b, hypothesis)\nprint(test)\n\n\n\n\nLa fuerte colinealidad introducida entre \\(x_1\\) y \\(x_2\\) (FIV alto) ha impactado negativamente en la inferencia estadística:\n\nInflación de la varianza: Los errores estándar de los coeficientes de \\(x_1\\) y \\(x_2\\) se han inflado significativamente.\nIntervalos de confianza anchos: La varianza inflada resulta en intervalos muy amplios.\nDificultad para rechazar \\(H_0\\) individual: El estadístico t se reduce, dificultando la detección de un efecto real.\nParadoja de la colinealidad: El test F conjunto logra rechazar \\(H_0\\), demostrando que el conjunto de variables es importante aunque individualmente no lo parezcan.\n\n\n\n\n\n# Parámetros\nset.seed(1234) \n# Generación variables (n=200)\nx1 &lt;- runif(200, min = 0, max = 50)\nv  &lt;- rnorm(200, mean = 0, sd = 1)\nx2 &lt;- x1 + v \nx3 &lt;- runif(200, min = 0, max = 50)\nu &lt;- rnorm(200, mean = 0, sd = 8)\ny &lt;- 2 + 0.5 * x1 + 0.5 * x2 + 1 * x3 + u\n\nmodelo2h &lt;- lm(y ~ x1 + x2 + x3)\nsummary(modelo2h)\n\n# FIVs para n=200\nvif(modelo2h)\n\n# Test F conjunto\nmodelo2hh &lt;- lm(y ~ x3)\nSRC_R &lt;- sum(resid(modelo2hh)^2) \nSRC_U &lt;- sum(resid(modelo2h)^2) \nF_est &lt;- ((SRC_R - SRC_U) / 2) / (SRC_U / df.residual(modelo2h))\nprint(F_est)\n\n\nLa colinealidad persiste: Los valores FIV para \\(x_1\\) y \\(x_2\\) siguen siendo extremadamente altos (~210), similares a los de \\(n=20\\). La colinealidad es estructural.\nLos estimadores han cambiado: Los errores estándar se han reducido considerablemente. Sin embargo, a pesar de aumentar \\(n\\) a 200, los p-values pueden seguir siendo altos si la colinealidad es extrema.\nLa expresión clave: \\[ Var(\\hat{\\beta}_j) = \\frac{\\sigma^2}{SCT_j (1-R^2_j)} = \\frac{\\sigma^2}{SCT_j} \\cdot FIV_j \\] El tamaño de la muestra afecta a la \\(SCT_j\\) (Suma de Cuadrados Totales de x). Cuanto mayor es \\(n\\), mayor es \\(SCT_j\\), lo que ayuda a reducir la varianza y contrarrestar el efecto del FIV alto."
  },
  {
    "objectID": "files/Ejercicios5Quarto.html#ejercicio-3",
    "href": "files/Ejercicios5Quarto.html#ejercicio-3",
    "title": "Ejercicios5",
    "section": "",
    "text": "education &lt;- read.csv(\"education.csv\")\n\n\n\n\n\\(\\beta_1\\) (sibs): NEGATIVO (-). En familias más grandes, hay menos recursos per cápita.\n\\(\\beta_2\\) (meduc): POSITIVO (+). Madres más educadas valoran más la educación.\n\\(\\beta_3\\) (feduc): POSITIVO (+). Padres más educados proveen mejor entorno académico.\n\n\n\n\n\nmodelo3b &lt;- lm(educ ~ sibs + meduc + feduc, data = education)\nsummary(modelo3b)\nlibrary(stargazer)\nstargazer(modelo3b, type = \"text\", \n          dep.var.labels = \"educación (en años)\", \n          covariate.labels = c(\"n. hermanos/as\", \"educación madre\", \"educación padre\", \"Intercept\"))\n\n\n\n\n\ncoefs &lt;- coef(modelo3b)\nerrores &lt;- summary(modelo3b)$coefficients[, \"Std. Error\"]\nr2 &lt;- summary(modelo3b)$r.squared\n\ncat(sprintf(\"educ = %.4f (%.4f) %.4f (%.4f) * sibs + %.4f (%.4f) * meduc + %.4f (%.4f) * feduc\\n\",\n            coefs[1], errores[1], coefs[2], errores[2], coefs[3], errores[3], coefs[4], errores[4]))\ncat(sprintf(\"R^2 = %.4f\\n\", r2))\n\n\\[educ = 10.3643 (0.3585) -0.0936 (0.0345) * sibs + 0.1308 (0.0327) * meduc + 0.2100 (0.0275) * feduc\\]\n\n\n\n\nt_b1 &lt;- abs(coefs[2] / errores[2])\nt_crit &lt;- qt(1 - 0.01/2, df = df.residual(modelo3b)) \nprint(t_b1)\nprint(t_crit)\n\nConclusión: Rechazamos \\(H_0\\).\n\n\n\n\n# F = t^2 para una restricción\nF_b1 &lt;- t_b1^2\nF_crit &lt;- qf(1 - 0.01, df1 = 1, df2 = df.residual(modelo3b))\nprint(F_b1)\nprint(F_crit)\n\nConclusión: Rechazamos \\(H_0\\).\n\n\n\n\nt_b2 &lt;- abs(coefs[3] / errores[3])\np_value_b2 &lt;- 2 * (1 - pt(t_b2, df = df.residual(modelo3b)))\nprint(p_value_b2)\n\n\n\n\n\nt_b3 &lt;- abs(coefs[4] / errores[4])\nlim &lt;- max(4, abs(t_b3) + 1)\ncurve(dt(x, df.residual(modelo3b)), -lim, lim, lwd=2, ylab=\"Densidad\", \n      main=paste(\"Estadístico t para feduc (t =\", round(t_b3, 4), \")\"))\nabline(v = c(t_b3, -t_b3), col=\"blue\", lwd=2, lty=2)\n\n\n\n\n\n*** \\(p &lt; 0.001\\) (Significativo al 0.1%)\n** \\(p &lt; 0.01\\) (Significativo al 1%)\n* \\(p &lt; 0.05\\) (Significativo al 5%)\n\n\n\n\n\nbeta1 &lt;- coefs[2]\nt_crit_95 &lt;- qt(1 - 0.025, df.residual(modelo3b))\nes_sibs &lt;- errores[2]\n\nIC_inf &lt;- beta1 - t_crit_95 * es_sibs\nIC_sup &lt;- beta1 + t_crit_95 * es_sibs\nprint(c(IC_inf, IC_sup))\n\n# Automático\nconfint(modelo3b, level = 0.95)[2,]\n\n\n\n\nEs probable que haya colinealidad porque las personas tienden a casarse con parejas de nivel educativo similar, lo que genera alta correlación entre meduc y feduc.\n\n\n\n\nvif(modelo3b)\n\n\n\n\n\nmodelo3l &lt;- lm(educ ~ 1, data=education) # Solo constante\n\nSRC_R &lt;- sum(resid(modelo3l)^2) # Es la SCT (Suma Cuadrados Total)\nSRC_U &lt;- sum(resid(modelo3b)^2)\n\nF_global &lt;- ((SRC_R - SRC_U) / 3) / (SRC_U / df.residual(modelo3b))\np_value_global &lt;- 1 - pf(F_global, df1=3, df2=df.residual(modelo3b))\nprint(F_global)\n\n# comprobamos con el summary\nprint(summary(modelo3b)$fstatistic)\n\n\n\n\n\nF_global_r2 &lt;- (r2 / 3) / ((1 - r2) / df.residual(modelo3b))\nprint(F_global_r2)\n\n\n\n\nQueremos saber si el efecto de la educación de la madre (beta2) es igual a lo del padre (beta3) \\[H_0: \\beta_2 - \\beta_3 = 0\\]\nSi queremos escribirlo como modelo restringido \\[ y = \\beta_0 + \\beta_1sibs + \\beta_2(meduc + feduc) + u\\]\n\n\n\nContrastamos si el efecto de la educación de la madre es igual al del padre.\n\n# Variable auxiliar suma\neducation$padres_sum &lt;- education$meduc + education$feduc\n# Modelo restringido: educ = b0 + b1*sibs + b2(padres_sum)\nmodelo3o &lt;- lm(educ ~ sibs + padres_sum, data=education)\n\nSRC_R_padres &lt;- sum(resid(modelo3o)^2)\nF_padres &lt;- ((SRC_R_padres - SRC_U) / 1) / (SRC_U / df.residual(modelo3b))\np_val_padres &lt;- 1 - pf(F_padres, df1=1, df2=df.residual(modelo3b))\n\nprint(F_padres)\nprint(p_val_padres)\n\nConclusión: El p-value es alto, no podemos rechazar \\(H_0\\). No hay evidencia de que los efectos sean distintos."
  },
  {
    "objectID": "files/Ejercicios5Quarto.html#ejercicio-4",
    "href": "files/Ejercicios5Quarto.html#ejercicio-4",
    "title": "Ejercicios5",
    "section": "",
    "text": "library(readr)\nvote1 &lt;- read.csv(\"vote1.csv\")\n\n\n\nExpectativas: \\(\\beta_1 &gt; 0\\), \\(\\beta_2 &lt; 0\\), \\(\\beta_3 &gt; 0\\)\n\nlibrary(stargazer)\nmodelo4a&lt;- lm(votA ~ expendA + expendB + prtystrA, data = vote1)\nstargazer(modelo4a, type = \"text\")\n\n\n\n\nGasto propio no afecta: \\[H_0: \\beta_1 = 0\\] Gasto rival no afecta: \\[H_0: \\beta_2 = 0\\]\nSi miramos los dos t estimados en summary(modelo4a), ya podemos rechazar las dos H_0\n\nsummary(modelo4a)\n\n\n\n\n\\[H_0: \\beta_1=-\\beta_2\\] Con esta hipotesis nula contrastamos si el coefficiente del gasto del candidato A tiene el efecto opuesto al gasto del candidato B\n\nhypothesis &lt;- c(\"expendA + expendB = 0\")\ntest &lt;- linearHypothesis (modelo4a, hypothesis)\nprint(test)\n\n# De manera manual\n# Error estándar de la suma: sqrt(Var(b1) + Var(b2) + 2*Cov(b1,b2))\nse_suma &lt;- sqrt(vcov(modelo4a)[\"expendA\",\"expendA\"] + \n                  vcov(modelo4a)[\"expendB\",\"expendB\"] + \n                  2*vcov(modelo4a)[\"expendA\",\"expendB\"])\n\nt_stat_manual &lt;- (modelo4a$coefficients[\"expendA\"] + modelo4a$coefficients[\"expendB\"]) / se_suma\np_val_manual &lt;- 2 * (1 - pt(abs(t_stat_manual), df.residual(modelo4a)))\n\nprint(t_stat_manual)\nprint(p_val_manual)\n\nNo podemos rechazar \\(H_0\\). Hay un problema de colinealidad!\n\n\n\n\nSRC_U &lt;- sum(resid(modelo4a)^2)\n# Creamos el modelo restringido: votA = beta0 + beta1(expendA+expendB)+beta2*prtystrA\nexpendAB &lt;-  vote1$expendA - vote1$expendB #creación variable combinada\nmodelo4d &lt;- lm(votA ~ expendAB + prtystrA, data = vote1)\nSRC_R &lt;- sum(resid(modelo4d)^2)\n\nF_est &lt;- ((SRC_R - SRC_U) / 1) / (SRC_U / df.residual(modelo4a))\nF_crit_d &lt;- qf(1 - 0.05, df1 = 1, df2 = df.residual(modelo4a)) # F crítico\np_value &lt;- 1 - pf(F_est, df1 = 1, df2 = df.residual(modelo4a))\nprint(p_value) # el mismo que con el test t"
  },
  {
    "objectID": "files/Ejercicios5Quarto.html#ejercicio-5",
    "href": "files/Ejercicios5Quarto.html#ejercicio-5",
    "title": "Ejercicios5",
    "section": "",
    "text": "Mide el efecto marginal de la penetración de redes sociales sobre la corrupción, ceteris paribus (manteniendo constante la renta per cápita).\n\n\n\n\nmodelo5b &lt;- lm(corruption ~ socialmediashare + gdppc, data = corruption)\nlibrary(stargazer)\nstargazer(modelo5b, type = \"text\")\n\nEl modelo explica el 74.8% de la variabilidad de la corrupción\n\\(\\beta_0\\): si un país tuviera 0 penetración de redes sociales y 0 renta per cápita, su índice de corrupción predicho sería de 100.6 puntos.\n\\(\\beta_1\\): manteniendo constante la renta per cápita, un aumento de 1 punto en la penetración de redes sociales está asociado a una disminución media de 0.0015 puntos en el índice de corrupción\n\\(\\beta_2\\): manteniendo constante la penetración de redes sociales, un aumento de 1 unidad en la variable de riqueza está asociado a una disminución media de 0.3803 puntos en el índice de corrupción\n\n\n\nExpectativa teórica\nEs razonable esperar que los países con mayor renta per cápita (más ricos) tengan mejores infraestructuras de telecomunicaciones, mayor acceso a tecnología (smartphones, ordenadores), y mayor alfabetización digital, lo que facilita una mayor penetración de las redes sociales\n\ncorrb1b2 &lt;- cor(corruption$socialmediashare, corruption$gdppc)\nprint(corrb1b2)\n\nConclusión: la correlación es positiva y muy alta\n\n\n\nSabemos que la riqueza (gdppc) reduce la corrupción (coeficiente negativo: -0.38) Hemos establecido en 5.c que la riqueza está correlacionada positivamente con las redes sociales. Por lo tanto, esperamos un sesgo de variable omitida negativo\nSi omitimos gdppc, el coeficiente de socialmediashare sufriría un sesgo negativo volviendose más negativo (en valor absoluto) de lo que realmente es. Comprobamolo:\n\nmodelo5d &lt;- lm(corruption ~ socialmediashare, data = corruption)\nsummary(modelo5d)\n\nPodemos confirmar nuestro asunto sobre la variable omitida"
  },
  {
    "objectID": "files/Ejercicios5Quarto.html#ejercicio-6",
    "href": "files/Ejercicios5Quarto.html#ejercicio-6",
    "title": "Ejercicios5",
    "section": "",
    "text": "rm(list = ls()) # limpiamos\nlibrary(readr)\nairfares &lt;- read.csv(\"airfares.csv\")\n\n\n\n\nmodelo6a &lt;- lm(log(fare) ~ log(disthm) + concen, data = airfares)\nsummary(modelo6a)\nlibrary(stargazer)\nstargazer(modelo6a, type = \"text\", dep.var.labels = \"Precio vuelos\", \n          covariate.labels = c(\"Distancia en 100 millas (log)\", \"Cuota de mercado (log)\", \"Intercepta\"))\n\n\n\n\nAl ser un modelo log-log, \\(\\beta_1\\) representa la elasticidad precio-distancia. Si la distancia aumenta un \\(1\\%\\), se estima que el precio del billete aumentará, en promedio, un \\(0.4\\%\\), manteniendo constante la concentración de mercado\n\n\n\n\nlibrary(car)\n# t = (beta1 - 0.5) / se(beta1)\nhypothesis &lt;- c(\"log(disthm) = 0.5\")\ntest &lt;- linearHypothesis(modelo6a, hypothesis)\nprint(test)\n# Nota: linearHypothesis usa el estadístico F. Recuerda que F = t^2 para 1 restricción\n\n# manual\nt_est &lt;- (summary(modelo6a)$coefficients[\"log(disthm)\",\"Estimate\"] - 0.5) / summary(modelo6a)$coefficients[\"log(disthm)\", \"Std. Error\"]\nprint(t_est)\nt_crit &lt;- qt(1 - 0.5/2, df = df.residual(modelo6a))\nprint(t_crit)\n\nRechazamos \\(H_0\\)\n\n\n\n\\(H_0: \\beta_1=0.5\\) contrasta si la elasticidad precio-distancia es exactamente \\(0.5\\) Es decir, si ante un aumento del \\(1\\%\\) en la distancia, el precio sube exactamente un \\(0.5\\%\\)\n\n\n\n\ncoef &lt;- as.numeric(modelo6a$coefficients[2])\nt_crit &lt;- qt(1 - 0.025, df = df.residual(modelo6a))\nes &lt;- as.numeric(summary(modelo6a)$coefficients[2, \"Std. Error\"])\n\n\\[CI=0.408621752 \\pm 1.962036 * 0.01776081\\]\n\nIC_inf &lt;- coef - t_crit * es\nIC_sup &lt;- coef + t_crit * es\n\nIC_manual &lt;- cbind(IC_inf, IC_sup) \nprint(IC_manual)\n\n# IC automáticos\nIC_95 &lt;- confint(modelo6a, level = 0.95)\nprint(IC_95)\n\nDado los valores del límite inferior y superior, podemos decir, con una confianza del \\(95\\%\\), que si la distancia augmenta en un \\(1\\%\\) el precio aumentará entre un \\(0.373774407\\) y un \\(0.443469097\\)\n\n\n\nSignifica que si tomáramos muchas muestras aleatorias diferentes de vuelos y construyéramos un intervalo de confianza para cada una el \\(95\\%\\) de esos intervalos contendrían el verdadero valor poblacional del parámetro \\(\\beta_1\\)\n\n\n\nPodemos mirar a la \\(t\\) sacada por el summary\n\nsummary(modelo6a)$coefficients[\"log(disthm)\", \"t value\"]\n\nCalculado manualmente como \\(\\hat{\\beta}_1 / se(\\hat{\\beta}_1)\\)\n\nt_est &lt;- (summary(modelo6a)$coefficients[\"log(disthm)\",\"Estimate\"] - 0) / summary(modelo6a)$coefficients[\"log(disthm)\", \"Std. Error\"]\nprint(t_est)\n\nt_crit &lt;- qt(1 - 0.5/2, df = df.residual(modelo6a))\nprint(t_crit)\n\n\\[|t_{est}| &gt; |t_{crit}| \\] Rechazamos \\(H_0\\)\n\np_val_manual &lt;- 2 * (1 - pt(abs(t_est), df.residual(modelo6a)))\nprint(p_val_manual)\n\n\n\n\n\nSRC_U &lt;- sum(resid(modelo6a)^2)\n\n# Creamos el modelo restringido: log(fare) = concen\nmodelo6h &lt;- lm(log(fare) ~ concen, data = airfares)\nSRC_R &lt;- sum(resid(modelo6h)^2)\n\nF_est &lt;- ((SRC_R - SRC_U) / 1) / (SRC_U / df.residual(modelo6a)) # exactamente t^2\nF_crit_d &lt;- qf(1 - 0.05, df1 = 1, df2 = df.residual(modelo6a)) # F crítico\np_value &lt;- 1 - pf(F_est, df1 = 1, df2 = df.residual(modelo6a))\nprint(p_value) # el mismo que con el test t\n\n\n\n\n\nmodelo6i &lt;- lm(log(fare) ~ log(disthm*100) + concen, data = airfares)\nsummary(modelo6i)\n\nMatematicamente, podemos dividir este logaritmo como ln(100) + ln(disthm) adonde el primer término se convierte ser una constante. Esta constante se suma a la constante De hecho, la intercepta será lo único que cambia con respeto al primer modelo."
  },
  {
    "objectID": "files/Ejercicios5Quarto.html#ejercicio-7",
    "href": "files/Ejercicios5Quarto.html#ejercicio-7",
    "title": "Ejercicios5",
    "section": "",
    "text": "rm(list = ls()) # limpiamos\nlibrary(readr)\nhprice2 &lt;- read.csv(\"hprice2.csv\")\n\n\n\n\nmodelo7a &lt;- lm(log(price) ~ rooms + log(nox) + log(dist) + stratio + crime, data = hprice2)\nsummary(modelo7a)\n\n\n\n\n\n\\(\\beta_1\\): positivo Más habitaciones implican una casa más grande, aumentando el precio\n\\(\\beta_2\\): negativo Más contaminación hace la zona menos deseable, bajando el precio\n\\(\\beta_3\\): negativo Estar lejos del centro reduce el valor. A veces puede ser positivo si estar lejos implica estar en suburbios limpios y tranquilos\n\\(\\beta_4\\): negativo Un ratio alto indica clases masificadas (= peor calidad escolar), reduciendo el valor de la zona\n\\(\\beta_5\\): negativo Más crimen hace la zona peligrosa y menos deseable\n\n\n\n\nTodos los estimadores han salido significativos al \\(1\\%\\)\n\n\n\nManteniendo constantes el resto de características, si una vivienda tiene una habitación más que otra (\\(\\Delta rooms = 1\\)), se estima que su precio será aproximadamente un \\(24.6\\%\\) mayor\n\n\n\nManteniendo constantes el resto de variables, si el nivel de contaminación aumenta un \\(1\\%\\), se estima que el precio de la vivienda disminuirá un \\(0.9\\%\\)\n\n\n\nBajo el supuesto de Media Condicional Cero (MLR.2): \\(E(u | nox, rooms, ...) = 0\\) Esto exige que la variable \\(log(nox)\\) sea exógena, es decir, que no esté correlacionada con el término de error \\(u\\). Es muy probable que existan variables omitidas en \\(u\\) (como la calidadestética del barrio, ruido, antigüedad de edificios) que estén correlacionadas con la contaminación. Podemos concluir que haya un sesgo de variable omitida"
  },
  {
    "objectID": "files/Ejercicios5Quarto.html#ejercicio-8",
    "href": "files/Ejercicios5Quarto.html#ejercicio-8",
    "title": "Ejercicios5",
    "section": "",
    "text": "rm(list = ls()) # limpiamos\nlibrary(readr)\nmicrosoft &lt;- read.csv(\"microsoft.csv\")\n\n\n\n\\[\\beta_6 = \\mathbb{E}(RPmsoft | m1=1, X) - \\mathbb{E}(RPmsoft | m1=0, X)\\] Donde \\(X\\) representa el resto de variables constantes (Ceteris Paribus) \\(\\beta_6\\) capta la diferencia esperada en la prima de riesgo de Microsoft en el mes de Enero (\\(m1=1\\)) comparada con el resto de meses del año, manteniendo constantes las condiciones de mercado aproximadas por los indices S&P500 y otras variables macroeconómicas\n\n\n\n\nmodelo8b &lt;- lm(RPmsoft ~ RPsandp + Dprod + Dinflation + Dterm + m1, data = microsoft)\nsummary(modelo8b)\n\n\n\n\n\nt_beta6 &lt;- coef(modelo8b)[\"m1\"] / coef(summary(modelo8b))[\"m1\", \"Std. Error\"]\nt_crit &lt;- qt(1 - 0.5/2, df = df.residual(modelo8b))\nprint(t_beta6)\nprint(t_crit)\n\n\\(H_0: \\beta_6 = 0\\) =&gt; No hay efecto diferencial en enero \\(H1: \\beta_6 \\neq 0\\) =&gt; Existe estacionalidad en enero\nNo rechazamos \\(H_0\\) La variable \\(m1\\) NO es significativa. No hay evidencia suficiente de un efecto diferencial\n\n\n\n\\[p-value = P(|T| &gt; |t_{est}|) = 2 * (1 - P(T &lt;= |t_{est}|))\\]\n\np_val_manual &lt;- 2 * (1 - pt(abs(t_beta6), df.residual(modelo8b)))\nprint(p_val_manual)\n\nGráfico:\n\n\n\nEl Efecto Enero sostiene que los rendimientos en enero tienden a ser más altos Aunque el signo pueda ser positivo, el efecto no es estadísticamente significativo No hay evidencia estadística suficiente para apoyar el Efecto Enero"
  },
  {
    "objectID": "files/Ejercicios5Quarto.html#ejercicio-9",
    "href": "files/Ejercicios5Quarto.html#ejercicio-9",
    "title": "Ejercicios5",
    "section": "",
    "text": "library(readr)\nrm(list = ls()) # limpiamos\ncps09 &lt;- read.csv(\"cps09subset.csv\")\n\n\n\n\nmodelo9a &lt;- lm(log(earnings) ~ ed + ex, data = cps09) # ecuación de Mincer!\nsummary(modelo9a)\n\n\n\n\nEn el Modelo 1, la relación entre el log del salario y la experiencia es lineal Si la experiencia aumenta de \\(1\\) a \\(2\\) años (\\(\\Delta ex = 1\\)), el salario esperado aumenta aproximadamente \\(1\\%\\), manteniendo la educación constante\n\n\n\nEn este Modelo 1, la variación esperada es exactamente la misma que en el apartado (b) El aumento estimado sigue siendo del \\(1\\%\\). El modelo lineal impone que el efecto marginal de la experiencia sea constante independientemente de cuánta experiencia se tenga acumulada\n\n\n\nEl término cuadratico sirve para capturar los rendimientos marginales decrecientes En la vida real, el salario suele subir rápido al inicio de la carrera (se aprende mucho) pero luego se estanca o crece más despacio (o incluso baja al final). Una línea recta no capta esto, pero una parábola (función cuadrática)\nEsperamos un signo positivo para \\(\\beta_2\\) y negativo para \\(\\beta_3\\). ¿Porqué? Para tener una forma de ‘U invertida’ (concava) que sube y luego baja (o se aplana), el coeficiente del la primera derivada (\\(\\beta_2\\)) tiene que ser positivo mientras el signo de la segunda derivada (\\(\\beta_3\\)) tiene que ser negativo\n\n\n\n\nmodelo9e &lt;- lm(log(earnings) ~ ed + ex + I(ex^2), data = cps09)\nsummary(modelo9e)\nlibrary(stargazer)\nstargazer(modelo9a, modelo9e, type = \"text\")\n\n\n\n\nEl signo de \\(\\beta_3\\) es negativo, tal como esperábamos Cómo cambian las interpretaciones de las preguntas (b) y (c)? Si la experiencia aumenta de \\(1\\) a \\(2\\) años (\\(\\Delta ex = 1\\)), el salario esperado aumenta \\[0.039*1+2*(-0.001)*1=0.037\\] Si la experiencia aumenta de \\(31\\) a \\(32\\) años (\\(\\Delta ex = 31\\)), el salario esperado aumenta \\[0.039*31+2*(-0.001)*31=1.147\\] ### 9.g) Significación \\(\\beta_3\\)\n\nt_beta3 &lt;- coef(modelo9e)[\"I(ex^2)\"] / coef(summary(modelo9e))[\"I(ex^2)\", \"Std. Error\"]\nt_crit &lt;- qt(1 - 0.01/2, df = df.residual(modelo9e)) # con alpha=0.1\nprint(t_beta3)\nprint(t_crit)\n\nEl regresor \\(ex^2\\) es significativo al \\(1\\%\\), confirmando la no linealidad"
  },
  {
    "objectID": "files/Ejercicios5Quarto.html#ejercicio-10",
    "href": "files/Ejercicios5Quarto.html#ejercicio-10",
    "title": "Ejercicios5",
    "section": "",
    "text": "rm(list = ls()) # limpiamos\nlibrary(readr)\ncoffee2m &lt;- read.csv(\"coffee2m.csv\")\n\n\n\nModelo completo \\[ln(Sales) = \\beta_0 + \\beta_1 * ln(price) + \\beta_2 * ad + \\beta_3(ad * log(price))\\]\nModelo con publicidad \\[\\mathbb{E}[sales|ad=1]= (\\beta_0 + \\beta_2) + (\\beta_2 + \\beta4) * log(price)\\]\nModelo sin publicidad \\[ \\mathbb{E} [sales|ad=0]= \\beta_0 + \\beta_1 * log(price) \\] ### 10.b) Parámetros elasticidad\nAquí miramos al modelo completo Elasticidad cuando hay publicidad \\[\\mathbb{E}[sales|price, ad=1] = \\beta_1 + \\beta_3 \\]\nElasticidad cuando no hay publicidad \\[\\mathbb{E}[sales|price, ad=0] = \\beta_1\\]\nSi \\(|\\beta_1 + \\beta_3| &gt; |\\beta_1|\\), la publicidad hace que la demanda sea más elásticay los consumidores se vuelven más sensibles al precio\nSi \\(|\\beta_1 + \\beta_3| &lt; |\\beta_1|\\), la publicidad hace que la demanda sea menos elásticay los consumidores se vuelven menos sensibles al precio\n\n\n\n\nmodelo10c &lt;- lm(log(sales) ~ log(price) * ad, data = coffee2m)\nlibrary(stargazer)\nstargazer(modelo10c, type = \"text\")\n\nCeteris paribus, las semanas con campaña publicitaria tienen en promedio, un volumen de ventas un \\(25.6\\%\\) mayor aproximadamente\nPero el cambio en la elasticidad no es estadísticamente significativo\n\n\n\nTenemos que contrastar ele efecto de la publicidad que afecta dos regresores: \\(\\beta_2\\) y \\(\\beta_3\\). Entonces nuestra hipotesis será \\[H_0: \\beta_2=0 \\quad \\beta_3=0\\]\nModelo restringido\n\nmodelo10d &lt;- lm(log(sales) ~ log(price), data = coffee2m)\n\nSRC_R &lt;- sum(resid(modelo10d)^2)\nSRC_U &lt;- sum(resid(modelo10c)^2)\n\nF_est &lt;- ((SRC_R - SRC_U) / 1) / (SRC_U / df.residual(modelo10c))\nF_crit_d &lt;- qf(1 - 0.01, df1 = 2, df2 = df.residual(modelo10c)) # F crítico con alpha=0.01\np_value &lt;- 1 - pf(F_est, df1 = 2, df2 = df.residual(modelo10c))\nprint(p_value) \n\nContraste \\(F\\) automatizado con linearHypothesis\n\nlibrary(car)\n\nhipotesis &lt;- c(\"ad = 0\", \"log(price):ad = 0\")\ntest_F_ad &lt;- linearHypothesis(modelo10c, hipotesis)\nprint(test_F_ad)\n\nEn este caso los comandos automatizados no nos permite establecer \\(\\alpha = 1\\%\\) Rechazamos \\(H_0\\)\n\n\n\n\nt_beta3 &lt;- coef(modelo10c)[\"log(price):ad\"] / coef(summary(modelo10c))[\"log(price):ad\", \"Std. Error\"]\nt_crit &lt;- qt(1 - 0.01/2, df = df.residual(modelo10c))\nprint(t_beta3)\nprint(t_crit)\n\nNo rechazamos \\(H_0\\)\n\n\n\n\nt_crit &lt;- qt(1 - 0.05/2, df = df.residual(modelo10c)) #t con alpha=0.05\n\nic_inf &lt;- coef(modelo10c)[\"log(price):ad\"] - t_crit * coef(summary(modelo10c))[\"log(price):ad\", \"Std. Error\"]\nic_sup &lt;- coef(modelo10c)[\"log(price):ad\"] + t_crit * coef(summary(modelo10c))[\"log(price):ad\", \"Std. Error\"]\nprint(c(ic_inf, ic_sup))\n\nIntervalos de confianza automatizado con confint\n\nIC_95 &lt;- confint(modelo10c, level = 0.95)\nprint(IC_95)\n\nEl intervalo de \\(\\beta_3\\) pasa por cero, entonces no podemos rechazar \\(H_0\\) ¿Cómo mejorar el experimento para medir \\(\\beta_3\\) con más precisión?\n\nAumentar el tamaño de la muestra (\\(n=16\\))\nAumentar la variabilidad de los precios (más días, más semanas = más precios observados)\nAumentar la submuestra de precios con publicidad (muestra más equilibrada)\nIncluir variables de control: temperatura, festivos, localización, etc."
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "CV",
    "section": "",
    "text": "You can find my latest academic CV below, or download it here."
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Codes",
    "section": "",
    "text": "Welcome to my code webpage. I am a proficient user of Stata but I also use RStudio for certain types of quantitative analyses. I can also use Eviews and Gretl.\nCurrently, I am learning Julia and then, I will move to Python."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Elian Giordano",
    "section": "",
    "text": "Hi! I’m Elian Giordano, a soon-to-be development economics graduate.\nIn the last two years, I attended a program called MEDEG, which stands for Master in Economic Development and Growth jointly offered by the Universidad Carlos III de Madrid, Spain, and Lund University, Sweden.\nCurrently, I am working as an adjunct lecturer at the Universidad Autónoma de Barcelona, Spain, where I teach Microeconomics and Econometrics I. At the same time I am supporting the project Digiwell led by Prof. Marinella Leone (Università di Pavia). Furthermore, I am part of the data replicator team led by Florian Oswald at the Journal of Political Economy.\nI am also a proud alumnus of IPLE, a bachelor’s programme at the Università degli Studi di Milano, Italy.\nApart from the academic fuss, I enjoy my time eating and texting with my friends. A glass of wine may be a good excuse to meet and talk about politics.\nDon’t take this website too seriously. For those who know me, I can be labelled as a stubborn and perfectionist person (or procrastinator, depending on the point of view). Bad combination.\nI still haven’t said the buzzword of my conversations, that is the place where I come from: Minori, Italy. It might be that you passed by while visiting the infamous Amalfi and Positano. It’s usually the case.\nI hope this wasn’t too long. At least you got a good overview before putting yourself in trouble by contacting me."
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Teaching",
    "section": "",
    "text": "Welcome to my teaching webpage. You will find here a list of ongoing or past courses with their related materials."
  },
  {
    "objectID": "courses.html#microeconomics",
    "href": "courses.html#microeconomics",
    "title": "Teaching",
    "section": "Microeconomics",
    "text": "Microeconomics\nCode: 102119 | Group: 10 | Degree: CiF View Syllabus\nI teach this course at the UAB Sabadell campus. Below you can find the complete set of exercises with solutions that I prepared for the students.\n\nExercise Sets with Solutions\n\nSet 1: Preferences and Utility\nSet 2: Budget Constraint\nSet 3: Consumer Optimal Choice\nSet 4: Consumer Demand\nSet 5: Technology\nSet 6: Profit Maximization\nSet 7: Cost Minimization and Cost Function\nSet 8: Short-Run and Long-Run Optimization\nSet 9: Market Equilibrium, Efficiency, and Tax Incidence\nSet 10: Monopoly and Efficiency\nSet 11: Monopoly and Price Discrimination\nSet 12: Oligopoly: Cournot and Bertrand\nSet 13: General Equilibrium"
  },
  {
    "objectID": "courses.html#econometrics-i",
    "href": "courses.html#econometrics-i",
    "title": "Teaching",
    "section": "Econometrics I",
    "text": "Econometrics I\nCode: 102308 | Group: 60 | Degree: ADE i Dret\nView Syllabus\nI took over instruction for the second half of this course. My section covers Topics 4, 5, and 6, focusing on practical implementation using R.\n\nLecture Slides\n\nTopic 4: The multiple regression model: estimation\nTopic 5: Linear regression analysis: inference\nTopic 6: Linear regression analysis: extensions\n\n\n\nR Lab Sessions with Solutions\nThese solutions were generated using Quarto.\n\nExercise Set 4: View Solution 4\nExercise Set 5: View Solution 5"
  },
  {
    "objectID": "files/Ejercicios4Quarto.html",
    "href": "files/Ejercicios4Quarto.html",
    "title": "Ejercicios4Quarto",
    "section": "",
    "text": "Si queremos que R ponga en automático la ubicación adonde estamos trabajando\nSi no, la podemos poner manualmente con este comando:\nPrimero debemos instalar los paquetes requeridos. Este paso se realiza una única vez. Posteriormente, en cada nueva sesión de trabajo, debemos cargar dichos paquetes utilizando la función library()\n\n\nImportamos el dataset\n\nmuestra4 &lt;- read_csv(\"muestra4.csv\")\n\n\n\nCálculo matricial \\((X'X)^{-1} * X'Y\\)\n\nY &lt;- as.matrix(muestra4$y) # Y vector (N x 1)\nX &lt;- as.matrix(cbind(1, muestra4$x1, muestra4$x2)) # X matriz: [Intercepta=1, x1, x2] (N x K)\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y # Aplicamos directamente la fórmula: beta_hat = (X'X)^-1 * X'y\nprint(beta_hat)\n\nComprobamos con el cálculo automatizado\n\nmodelo1a &lt;- lm(y ~ x1 + x2, data=muestra4) # en la RLM ponemos \"+\" y el nombre de la nueva variable\nprint(modelo1a) # Coeficientes\nsummary(modelo1a) # Coeficientes, SE, t-stat, R^2, etc.\n\n\n\n\nEstablecemos los parámetros del modelo (\\(n\\), \\(K\\))\n\nn &lt;- nrow(muestra4) # número observaciones       \nK &lt;- ncol(X) # número parámetros (incluyendo intercepta) \ngl_model &lt;- n - K # grados de libertad (gl)\n\nCálculo de Residuos, SCR y Varianza del Error\n\nY_hat &lt;- X %*% beta_hat \nU_hat &lt;- Y - Y_hat\nSCR &lt;- t(U_hat) %*% U_hat # Suma de Cuadrados de los Residuos\nsigma2_hat &lt;- SCR / gl_model # Estimador insesgado de la varianza del error\n\nCálculo de la Matriz de Varianza-Covarianza y Errores Estándar\n\nVar_beta_hat &lt;- as.numeric(sigma2_hat) * solve(t(X) %*% X) # Var_beta_hat = sigma2_hat * (X'X)^-1\nprint(Var_beta_hat) # Las varianzas (Var(beta_j)) están en la diagonal. Las demás son covarianzas.\nse_beta_hat &lt;- sqrt(diag(Var_beta_hat))\nprint(se_beta_hat)\n\nComprobamos con el cálculo automatizado\n\nsummary(modelo1a)$coefficients[, \"Std. Error\"] #columna \n\n\n\n\nDe manera sencilla y rápida\n\nstargazer(modelo1a, type = \"text\")\n\nCon título, nombre variables y estrellas personalizadas\n\nstargazer(modelo1a, type = \"text\", \n          title = \"Modelo de Regresión Múltiple: y en función de x1 y x2\",\n          dep.var.labels = \"y\",\n          covariate.labels = c(\"x1\", \"x2\", \"Intercept\"),\n          star.cutoffs = c(0.1, 0.05, 0.01))\n\n\n\n\n\ninv_XtX &lt;- solve(t(X) %*% X) # Matriz X'X\ninv_XtX_22 &lt;- inv_XtX[2, 2] # El elemento para beta_1 (segundo parámetro) es la posición [2, 2]\nprint(inv_XtX_22)\nx1_vector &lt;- as.matrix(muestra4$x1) # vector de sólo x1\nx1_mean &lt;- mean(x1_vector) # Cálculo de SQT_1\nSQT_1 &lt;- t(x1_vector - x1_mean) %*% (x1_vector - x1_mean) # SQT_1 = Suma de (x1 - mean(x1))^2\nprint(SQT_1)\n\nPara encontrar \\(R^2_1\\) tenemos que estimar un MCO de \\(x_1\\) sobre los otros regresores. Modelo auxiliar: \\[x_1 = \\delta_0 + \\delta_2*x_2 + v\\]\n\nmodel_aux &lt;- lm(x1 ~ x2, data = muestra4) # Modelo Auxiliar\nR_squared_1 &lt;- summary(model_aux)$r.squared\nprint(R_squared_1)\n\nPara encontrar la descomposición de la varianza aplicamos la fórmula \\[\\frac{1}{SQT_1} * \\frac{1}{(1 - R_1^2)}\\]\n\ndecomposicion_var_b1 &lt;- (1 / as.numeric(SQT_1)) * (1 / (1 - R_squared_1)) #(1 / SQT_1) * (1 / (1 - R_1^2))\nprint(decomposicion_var_b1)\n\nComprobamos con la varianza que se encontraba en la matriz de la varianza\n\nprint(inv_XtX[2, 2]) #comprobamos\n\n\n\n\n\n\n\nLa matriz de regresores \\(X\\) tiene la forma: \\(X = [1, x_1, 2*x_1]\\) La tercera columna (col_2) es una función lineal exacta de la segunda (col_2 = 2 * col_1). Las columnas de \\(X\\) no son linealmente independientes. A causa de esta dependencia, la matriz (\\(X'X\\)) se vuelve “singular”, lo que significa que su determinante es \\(0\\). Una matriz singular (con determinante \\(0\\)) NO tiene inversa. Dado que la fórmula requiere calcular \\((X'X)^{-1}\\), y esta inversa no existe, es matemáticamente imposible calcular las estimaciones.\n\nrm()\nn &lt;- 50             # Número de observaciones\nbeta0 &lt;- 2\nbeta1 &lt;- 1.5\nbeta2 &lt;- 2.0\nsigma_u &lt;- 5        # Desviación estándar del error\n\nset.seed(42) # Para reproducibilidad\n\n\nx1 &lt;- runif(n, min = 10, max = 20)\nx2 &lt;- 2 * x1 #x2 es una función de x1\n\nu &lt;- rnorm(n, mean = 0, sd = sigma_u) \ny &lt;- beta0 + beta1 * x1 + beta2 * x2 + u #calculado como valor predicho y con los parametros establecidos arriba\n\n# Bases de datos\ndatos &lt;- data.frame(y, x1, x2)\n\nEstimamos la correlación\n\ncorr1 &lt;- cor(datos$x1, datos$x2)\nprint(corr1)\n\nLa correlación es \\(1.0\\), lo que confirma colinearidad perfecta\nR es inteligente y detecta la colinearidad perfecta. Para evitar el fallo en \\((X'X)^{-1}\\), elimina automáticamente una de las variables\n\nmodelo_colineal &lt;- lm(y ~ x1 + x2, data = datos)\nsummary(modelo_colineal)\n\n\n\n\n\\(R_1^2\\) es el R-cuadrado de la regresión auxiliar de \\(x_1\\) sobre \\(x_2\\). \\(R_2^2\\) es el R-cuadrado de la regresión auxiliar de \\(x_2\\) sobre \\(x_1\\).\nDada la condición \\(x_2 = 2 * x_1\\), \\(x_1\\) explica el \\(100\\%\\) de la variabilidad de \\(x_2\\) (y viceversa). Por lo tanto, si hiciéramos la regresión auxiliar de \\(x_2\\) sobre \\(x_1\\), obtendríamos un ajuste perfecto.\nEsto implica que \\(R_1^2 = 1\\) y \\(R_2^2 = 1\\).\nAl sustituir este valor en las fórmulas de la varianza: \\[var(\\beta_1) = \\frac{\\sigma_1}{(1 - 1)} = \\frac{\\sigma_1}{0}\\] \\[var(\\beta_2) = \\frac{\\sigma_2}{(1 - 1)} = \\frac{\\sigma_2}{0}\\]\nLa división por cero es indefinida Una varianza infinita implica una precisión nula. Los errores estándar serían infinitos, haciendo imposible distinguir los valores de \\(\\beta_1\\) y \\(\\beta_2\\) de cero o de cualquier otro valor.\n\nmodel_aux_x1 &lt;- lm(x1 ~ x2, data = datos)\nprint(model_aux_x1) #resultados\nR_squared_1 &lt;- summary(model_aux_x1)$r.squared\n\n\nFIV_x1 &lt;- 1 / (1 - R_squared_1)\nprint(FIV_x1)\n\nDebido a errores de punto flotante, \\(R^2_1\\) puede ser ligeramente menor que \\(1\\), pero la intención es demostrar la división por cero.\nSi \\(R^2_1\\) fuera exactamente \\(1\\), el denominador sería \\(0\\), y el \\(FIV\\) sería infinito. Cuando \\(R^2_1\\) es \\(1.0\\), R devuelve ‘Inf’. Por ejemplo, si lo forzamos a 1: \\(1 / (1 - 1)\\) Esto resulta en Inf\nLa varianza \\(Var(\\beta_1|X)\\) es proporcional al \\(FIV\\). Como \\(R^2_1\\) es \\(1\\), el \\(FIV\\) tiende a infinito. Por lo tanto, la varianza de los estimadores \\(Var(\\beta_1|X\\) y \\(Var(\\beta_2|X)\\) es infinita, confirmando que la información de la muestra no permite estimar los efectos individuales.\n\n\n\n\n\n\n\nset.seed(1010)\nn &lt;- 100           # Número de observaciones\nbeta0 &lt;- 2         # Verdadero intercepto\nbeta1 &lt;- 1.5       # Verdadero efecto de x1\nbeta2 &lt;- 2.0       # Verdadero efecto de x2\nsigma_u &lt;- 3       # Desviación estándar del error siendo que la varianza es 9\n\n\nx1 &lt;- runif(n, min = 0, max = 100) \nx2 &lt;- runif(n, min = 0, max = 100) \nu &lt;- rnorm(n, mean = 0, sd = sigma_u) \ny &lt;- beta0 + beta1 * x1 + beta2 * x2 + u \nsimulacion1a &lt;- data.frame(y, x1, x2)\n\n\n\n\n\nmodelo3b &lt;- lm(y ~ x1 + x2, data = simulacion1a) #estimación\nsummary(modelo3b) #x1 subestimado y x2 sobreestimado\n\n\n\n\n\nmodelo_aux_x1 &lt;- lm(x1 ~ x2, data = simulacion1a)\nR2_1 &lt;- summary(modelo_aux_x1)$r.squared\nFIV1 &lt;- 1 / (1 - R2_1)\nprint(FIV1)\n\nmodelo_aux_x2 &lt;- lm(x2 ~ x1, data = simulacion1a)\nR2_2 &lt;- summary(modelo_aux_x2)$r.squared\nFIV2 &lt;- 1 / (1 - R2_2)\nprint(FIV2)\n\nEl valor bajo y cerca de \\(1\\) no nos sorprende. En el MGD(1), \\(x_1\\) y \\(x_2\\) se generaron de forma i.i.d. (independiente e idénticamente distribuidas) y uniforme, por lo que se espera que la correlación entre ellas sea cercana a cero\nControlamos con el paquete car si lo hemos calculado bien\n\nlibrary(car)\nvif(modelo3b)\n\n\n\n\n\nbeta0 &lt;- 2\nbeta1 &lt;- 0.5\nbeta2 &lt;- 0.25\nsigma_v &lt;- 7 # Raíz de la varianza (49)\nset.seed(1010)\n\nx1 &lt;- runif(n, min = 0, max = 100) \nv &lt;- rnorm(n, mean = 0, sd = sigma_v)\nx2 &lt;- 2 * x1 + v \n\nu &lt;- rnorm(n, mean = 0, sd = sigma_u)\ny &lt;- beta0 + beta1 * x1 + beta2 * x2 + u\n\nsimulacion1b &lt;- data.frame(y, x1, x2)\n\n\nmodelo3d &lt;- lm(y ~ x1 + x2, data = simulacion1b)\nsummary(modelo3d)\n\n\nmodelo_aux_x1 &lt;- lm(x1 ~ x2, data = simulacion1b)\nR2_1 &lt;- summary(modelo_aux_x1)$r.squared\nFIV1 &lt;- 1 / (1 - R2_1)\nprint(FIV1)\n\nmodelo_aux_x2 &lt;- lm(x2 ~ x1, data = simulacion1b)\nR2_2 &lt;- summary(modelo_aux_x2)$r.squared\nFIV2 &lt;- 1 / (1 - R2_2)\nprint(FIV2)\n\nLos FIVs son muy altos debidos a los \\(R^2\\) muy altos. Hay colinearidad\n\n\n\n\nn &lt;- 4000\nset.seed(1010)\n\nx1 &lt;- runif(n, min = 0, max = 100) \nv &lt;- rnorm(n, mean = 0, sd = sigma_v)\nx2 &lt;- 2 * x1 + v \n\nu &lt;- rnorm(n, mean = 0, sd = sigma_u)\ny &lt;- beta0 + beta1 * x1 + beta2 * x2 + u\n\nsimulacion1c &lt;- data.frame(y, x1, x2)\n\n\nmodelo3e &lt;- lm(y ~ x1 + x2, data = simulacion1c)\nsummary(modelo3e)\n\n\nmodelo_aux_x1 &lt;- lm(x1 ~ x2, data = simulacion1c)\nR2_1 &lt;- summary(modelo_aux_x1)$r.squared\nFIV1 &lt;- 1 / (1 - R2_1)\nprint(FIV1)\n\nmodelo_aux_x2 &lt;- lm(x2 ~ x1, data = simulacion1c)\nR2_2 &lt;- summary(modelo_aux_x2)$r.squared\nFIV2 &lt;- 1 / (1 - R2_2)\nprint(FIV2)\n\nLos FIVs son muy altos debidos a los \\(R^2\\) muy altos. Hay colinearidad pero los estimadores son más cercanos a los verdaderos porqué ahora son consistentes debido a un \\(N\\) muy alto\n\n\n\n\n\n\nEl resultado debe ser \\(10,000\\) estimaciones para cada parámetro.\n\n\n\n\nset.seed(1010)\nreps &lt;- 10000\nn &lt;- 40\nbeta0 &lt;- 2\nbeta1 &lt;- 1.5\nbeta2 &lt;- 2.0\nsigma_u &lt;- 3\n\n\nmontecarlo1 &lt;- matrix(NA, nrow = reps, ncol = 3, \n                      dimnames = list(NULL, c(\"b0\", \"b1\", \"b2\")))\n\nx1 &lt;- runif(n, min = 0, max = 100) # x1 ~ U(0, 100)\nx2 &lt;- runif(n, min = 0, max = 100) # x2 ~ U(0, 100)\n\nfor (r in 1:reps) {\n  u &lt;- rnorm(n, mean = 0, sd = sigma_u) \n  y &lt;- beta0 + beta1 * x1 + beta2 * x2 + u\n  \n  simulacion4b &lt;- data.frame(y, x1, x2) #base de datos con y\n  modelo4b &lt;- lm(y ~ x1 + x2, data = simulacion4b) #modelo\n  \n  montecarlo1[r, ] &lt;- coef(modelo4b) #guardamos los coeficientes en la matriz\n}\n\nhead(montecarlo1)\n\n\nmedia_mc &lt;- colMeans(montecarlo1) #calculamos el promedio de los coeficientes obtenidos\nsd_mc &lt;- apply(montecarlo1, 2, sd) #calculamos la desviación estándard de los coeficientes obtenidos\nprint(media_mc) #parámetros finales\nprint(sd_mc) #desviación estándards finales\n\n\n\n\n\nestimaciones_b1 &lt;- montecarlo1[, \"b1\"] #guardamos los beta1 estimados\n\n# Convertimos las estimaciones de b1 en un data frame para ggplot2\nb1_mc &lt;- data.frame(b1_hat = estimaciones_b1)\n\nGráfico sencillo\n\nhist(estimaciones_b1, \n     breaks = 50, \n     freq = FALSE, # Queremos la densidad en el eje vertical\n     main = \"Distribución Muestral de b1\",\n     xlab = \"Valor Estimado de b1\")\n\nGrafico complejo con ggplot2\n\n\n\n\nset.seed(1010)\nreps &lt;- 10000\nn &lt;- 40\nbeta0 &lt;- 2\nbeta1 &lt;- 0.5\nbeta2 &lt;- 0.25\nsigma_v &lt;- 7 # Raíz de la varianza (49)\n\n\nmontecarlo2 &lt;- matrix(NA, nrow = reps, ncol = 3, \n                      dimnames = list(NULL, c(\"b0\", \"b1\", \"b2\")))\n\nx1 &lt;- runif(n, min = 0, max = 100) # x1 ~ U(0, 100)\nv &lt;- rnorm(n, mean = 0, sd = sigma_v)\nx2 &lt;- 2 * x1 + v \n\nfor (r in 1:reps) {\n  u &lt;- rnorm(n, mean = 0, sd = sigma_u) \n  y &lt;- beta0 + beta1 * x1 + beta2 * x2 + u\n  \n  simulacion4d &lt;- data.frame(y, x1, x2) #base de datos con y\n  modelo4d &lt;- lm(y ~ x1 + x2, data = simulacion4d) #modelo\n  \n  montecarlo2[r, ] &lt;- coef(modelo4d) #guardamos los coeficientes en la matriz\n}\n\nmedia_mc &lt;- colMeans(montecarlo2) #calculamos el promedio de los coeficientes obtenidos\nsd_mc &lt;- apply(montecarlo2, 2, sd) #calculamos la desviación estándard de los coeficientes obtenidos\n\nprint(media_mc) #parámetros finales\nprint(sd_mc) #desviación estándards finales\n\nestimaciones_b1 &lt;- montecarlo2[, \"b1\"] #guardamos los beta1 estimados\n\nGráfico\n\n# Convertimos las estimaciones de b1 en un data frame para ggplot2\nb1_mc &lt;- data.frame(b1_hat = estimaciones_b1)\n\n# Gráfico sencillo\nhist(estimaciones_b1, \n     breaks = 50, \n     freq = FALSE, # Queremos la densidad en el eje vertical\n     main = \"Distribución Muestral de b1\",\n     xlab = \"Valor Estimado de b1\")\n\nInsesgadez: Ambas distribuciones están centradas en \\(0.5\\). La multicolinearidad NO provoca sesgo.\nVarianza: El histograma MGD 2 es más disperso y achatado que el MGD 1. Esto demuestra que la alta colinearidad infla la varianza, haciendo las estimaciones menos precisas\n\n\n\n\n\nlibrary(readr)\nairfares &lt;- read_csv(\"airfares.csv\")\n\n\n\n\nmodelo5a &lt;- lm(fare ~ disthm + concen, data = airfares)\nlibrary(stargazer)\nstargazer(modelo5a, type = \"text\")\n\n\n\n\n\nY &lt;- as.matrix(airfares$fare) # Y vector (N x 1)\nX &lt;- as.matrix(cbind(1, airfares$disthm, airfares$concen)) # X matriz: [Intercepta=1, x1, x2] (N x K)\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y # Aplicamos directamente la fórmula: beta_hat = (X'X)^-1 * X'y\nprint(beta_hat)\n\n# Parámetros del modelo (n, K)\nn &lt;- nrow(X) # número observaciones       \nK &lt;- ncol(X) # número parámetros (incluyendo intercepta) \n\n\nY_hat &lt;- X %*% beta_hat \nU_hat &lt;- Y - Y_hat\nSCR &lt;- t(U_hat) %*% U_hat # Suma de Cuadrados de los Residuos\n\n\nSCT &lt;- t(Y - mean(Y)) %*% (Y - mean(Y))\nR2 &lt;- 1 - (SCR / SCT)\nprint(R2)\nR2_adj &lt;- 1 - (1 - R2) * (n - 1) / (n - K)\nprint(R2_adj)\n\n\ngl_model &lt;- n - K # grados de libertad (gl)\nsigma2_hat &lt;- SCR / gl_model # Estimador insesgado de la varianza del error\n\n# Cálculo de la Matriz de Varianza-Covarianza y Errores Estándar\nVar_beta_hat &lt;- as.numeric(sigma2_hat) * solve(t(X) %*% X) # Var_beta_hat = sigma2_hat * (X'X)^-1\nse_beta_hat &lt;- sqrt(diag(Var_beta_hat))\nprint(se_beta_hat)\n\n\n\n\n\\[\\begin{align*}\n\\widehat{fare} &= 62.688 + 8.635 \\cdot disthm + 0.663 \\cdot concen \\\\\n& \\quad (8.828) \\quad (0.342) \\quad \\quad \\quad \\quad (0.106) \\\\\nn &= 1149 \\\\\nR^2 &= 0.380\n\\end{align*}\\]\n\\(\\hat{Y}\\) representa el vector con los valores ajustados\n\n\n\nSignos:\n\n\\(\\beta_1\\) (disthm): El coeficiente es positivo (\\(+8.635\\)). Era esperable, ya que a mayor distancia, mayor precio del billete (ceteris paribus).\n\\(\\beta_2\\) (concen): El coeficiente es positivo (\\(+0.663\\)). También era esperable; a mayor concentración (menos competencia), mayor precio del billete (ceteris paribus).\n\nBondad del ajuste: El \\(R\\^2\\) es \\(0.380\\) (\\(38\\%\\)). Esto significa que la distancia y la concentración explican el \\(38\\%\\) de la variabilidad en las tarifas. Un \\(38\\%\\) es un ajuste moderado pero razonable. Indica que hay otros factores importantes (\\(62\\%\\)) que afectan al precio y no están en el modelo.\n\n\n\nManteniendo constante el nivel de concentración del mercado (concen), un aumento de una unidad en disthm (es decir, un aumento de \\(100\\) millas en la distancia del vuelo) está asociado con un aumento medio de \\(8.635\\) dólares en el precio del billete.\n\n\n\n\n#Estimación \nmodelo5e &lt;- lm(fare ~ disthm, data = airfares)\n\n# Comparación\nstargazer(modelo5a, modelo5e, type = \"text\",\n          column.labels = c(\"Modelo (1)\", \"Modelo (2)\"),\n          keep.stat = c(\"n\", \"rsq\", \"adj.rsq\"))\n\ni. Comparación R^2\nPor definición matemática, al añadir una variable a un modelo (pasar de Modelo(2) a Modelo(1)), la Suma de Cuadrados de los Residuos (\\(SCR\\)) solo puede disminuir o quedarse igual, nunca aumentar Dado que \\(R^2 = 1 - (SCR / SCT)\\) y la Suma de Cuadrados Totales (\\(SCT\\)) es idéntica para ambos modelos, el modelo con la \\(SCR\\) más baja (Modelo(1)) tendrá necesariamente el \\(R^2\\) más alto\nii. Comparación \\(\\bar{R}^2\\)\nEn el Modelo(1), el valor obtenido fue: \\(\\bar{R}^2 = 0.379\\) En el Modelo(2), el valor obtenido es más bajo y igual a \\(R^2\\) porqué es una regresión simple.\niii. Elección del Modelo\nEl \\(\\bar{R}^2\\) del Modelo (1) (\\(0.379\\)) es superior al \\(\\bar{R}^2\\) del Modelo(2) (\\(0.354\\)). Esto indica que la variable concen es relevante y aporta suficiente poder explicativo como para justificar su inclusión.\nComprobamos si el Modelo (2) sufre de Variable omitida utilizando la formula del modelo auxiliar y el \\(FIV\\)\nRegresión auxiliar para comprobar la correlación y el signo del sesgo:\n\nmodelo_auxiliar &lt;- lm(concen ~ disthm, data = airfares)\nsummary(modelo_auxiliar)\n\nPara analizar el sesgo el Modelo(2), necesitamos dos informaciones: Del Modelo(1) sabemos que \\(\\beta_2\\) (coeficiente de concen) es POSITIVO En el modelo auxiliar vemos que el coeficiente de disthm es NEGATIVO \\[Sesgo = (Positivo) * (Negativo) = NEGATIVO\\]\nEl estimador de disthm en el Modelo(2) sufre de un sesgo negativo"
  },
  {
    "objectID": "files/Ejercicios4Quarto.html#ejercicio-1",
    "href": "files/Ejercicios4Quarto.html#ejercicio-1",
    "title": "Ejercicios4Quarto",
    "section": "",
    "text": "Importamos el dataset\n\nmuestra4 &lt;- read_csv(\"muestra4.csv\")\n\n\n\nCálculo matricial \\((X'X)^{-1} * X'Y\\)\n\nY &lt;- as.matrix(muestra4$y) # Y vector (N x 1)\nX &lt;- as.matrix(cbind(1, muestra4$x1, muestra4$x2)) # X matriz: [Intercepta=1, x1, x2] (N x K)\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y # Aplicamos directamente la fórmula: beta_hat = (X'X)^-1 * X'y\nprint(beta_hat)\n\nComprobamos con el cálculo automatizado\n\nmodelo1a &lt;- lm(y ~ x1 + x2, data=muestra4) # en la RLM ponemos \"+\" y el nombre de la nueva variable\nprint(modelo1a) # Coeficientes\nsummary(modelo1a) # Coeficientes, SE, t-stat, R^2, etc.\n\n\n\n\nEstablecemos los parámetros del modelo (\\(n\\), \\(K\\))\n\nn &lt;- nrow(muestra4) # número observaciones       \nK &lt;- ncol(X) # número parámetros (incluyendo intercepta) \ngl_model &lt;- n - K # grados de libertad (gl)\n\nCálculo de Residuos, SCR y Varianza del Error\n\nY_hat &lt;- X %*% beta_hat \nU_hat &lt;- Y - Y_hat\nSCR &lt;- t(U_hat) %*% U_hat # Suma de Cuadrados de los Residuos\nsigma2_hat &lt;- SCR / gl_model # Estimador insesgado de la varianza del error\n\nCálculo de la Matriz de Varianza-Covarianza y Errores Estándar\n\nVar_beta_hat &lt;- as.numeric(sigma2_hat) * solve(t(X) %*% X) # Var_beta_hat = sigma2_hat * (X'X)^-1\nprint(Var_beta_hat) # Las varianzas (Var(beta_j)) están en la diagonal. Las demás son covarianzas.\nse_beta_hat &lt;- sqrt(diag(Var_beta_hat))\nprint(se_beta_hat)\n\nComprobamos con el cálculo automatizado\n\nsummary(modelo1a)$coefficients[, \"Std. Error\"] #columna \n\n\n\n\nDe manera sencilla y rápida\n\nstargazer(modelo1a, type = \"text\")\n\nCon título, nombre variables y estrellas personalizadas\n\nstargazer(modelo1a, type = \"text\", \n          title = \"Modelo de Regresión Múltiple: y en función de x1 y x2\",\n          dep.var.labels = \"y\",\n          covariate.labels = c(\"x1\", \"x2\", \"Intercept\"),\n          star.cutoffs = c(0.1, 0.05, 0.01))\n\n\n\n\n\ninv_XtX &lt;- solve(t(X) %*% X) # Matriz X'X\ninv_XtX_22 &lt;- inv_XtX[2, 2] # El elemento para beta_1 (segundo parámetro) es la posición [2, 2]\nprint(inv_XtX_22)\nx1_vector &lt;- as.matrix(muestra4$x1) # vector de sólo x1\nx1_mean &lt;- mean(x1_vector) # Cálculo de SQT_1\nSQT_1 &lt;- t(x1_vector - x1_mean) %*% (x1_vector - x1_mean) # SQT_1 = Suma de (x1 - mean(x1))^2\nprint(SQT_1)\n\nPara encontrar \\(R^2_1\\) tenemos que estimar un MCO de \\(x_1\\) sobre los otros regresores. Modelo auxiliar: \\[x_1 = \\delta_0 + \\delta_2*x_2 + v\\]\n\nmodel_aux &lt;- lm(x1 ~ x2, data = muestra4) # Modelo Auxiliar\nR_squared_1 &lt;- summary(model_aux)$r.squared\nprint(R_squared_1)\n\nPara encontrar la descomposición de la varianza aplicamos la fórmula \\[\\frac{1}{SQT_1} * \\frac{1}{(1 - R_1^2)}\\]\n\ndecomposicion_var_b1 &lt;- (1 / as.numeric(SQT_1)) * (1 / (1 - R_squared_1)) #(1 / SQT_1) * (1 / (1 - R_1^2))\nprint(decomposicion_var_b1)\n\nComprobamos con la varianza que se encontraba en la matriz de la varianza\n\nprint(inv_XtX[2, 2]) #comprobamos"
  },
  {
    "objectID": "files/Ejercicios4Quarto.html#ejercicio-2",
    "href": "files/Ejercicios4Quarto.html#ejercicio-2",
    "title": "Ejercicios4Quarto",
    "section": "",
    "text": "La matriz de regresores \\(X\\) tiene la forma: \\(X = [1, x_1, 2*x_1]\\) La tercera columna (col_2) es una función lineal exacta de la segunda (col_2 = 2 * col_1). Las columnas de \\(X\\) no son linealmente independientes. A causa de esta dependencia, la matriz (\\(X'X\\)) se vuelve “singular”, lo que significa que su determinante es \\(0\\). Una matriz singular (con determinante \\(0\\)) NO tiene inversa. Dado que la fórmula requiere calcular \\((X'X)^{-1}\\), y esta inversa no existe, es matemáticamente imposible calcular las estimaciones.\n\nrm()\nn &lt;- 50             # Número de observaciones\nbeta0 &lt;- 2\nbeta1 &lt;- 1.5\nbeta2 &lt;- 2.0\nsigma_u &lt;- 5        # Desviación estándar del error\n\nset.seed(42) # Para reproducibilidad\n\n\nx1 &lt;- runif(n, min = 10, max = 20)\nx2 &lt;- 2 * x1 #x2 es una función de x1\n\nu &lt;- rnorm(n, mean = 0, sd = sigma_u) \ny &lt;- beta0 + beta1 * x1 + beta2 * x2 + u #calculado como valor predicho y con los parametros establecidos arriba\n\n# Bases de datos\ndatos &lt;- data.frame(y, x1, x2)\n\nEstimamos la correlación\n\ncorr1 &lt;- cor(datos$x1, datos$x2)\nprint(corr1)\n\nLa correlación es \\(1.0\\), lo que confirma colinearidad perfecta\nR es inteligente y detecta la colinearidad perfecta. Para evitar el fallo en \\((X'X)^{-1}\\), elimina automáticamente una de las variables\n\nmodelo_colineal &lt;- lm(y ~ x1 + x2, data = datos)\nsummary(modelo_colineal)\n\n\n\n\n\\(R_1^2\\) es el R-cuadrado de la regresión auxiliar de \\(x_1\\) sobre \\(x_2\\). \\(R_2^2\\) es el R-cuadrado de la regresión auxiliar de \\(x_2\\) sobre \\(x_1\\).\nDada la condición \\(x_2 = 2 * x_1\\), \\(x_1\\) explica el \\(100\\%\\) de la variabilidad de \\(x_2\\) (y viceversa). Por lo tanto, si hiciéramos la regresión auxiliar de \\(x_2\\) sobre \\(x_1\\), obtendríamos un ajuste perfecto.\nEsto implica que \\(R_1^2 = 1\\) y \\(R_2^2 = 1\\).\nAl sustituir este valor en las fórmulas de la varianza: \\[var(\\beta_1) = \\frac{\\sigma_1}{(1 - 1)} = \\frac{\\sigma_1}{0}\\] \\[var(\\beta_2) = \\frac{\\sigma_2}{(1 - 1)} = \\frac{\\sigma_2}{0}\\]\nLa división por cero es indefinida Una varianza infinita implica una precisión nula. Los errores estándar serían infinitos, haciendo imposible distinguir los valores de \\(\\beta_1\\) y \\(\\beta_2\\) de cero o de cualquier otro valor.\n\nmodel_aux_x1 &lt;- lm(x1 ~ x2, data = datos)\nprint(model_aux_x1) #resultados\nR_squared_1 &lt;- summary(model_aux_x1)$r.squared\n\n\nFIV_x1 &lt;- 1 / (1 - R_squared_1)\nprint(FIV_x1)\n\nDebido a errores de punto flotante, \\(R^2_1\\) puede ser ligeramente menor que \\(1\\), pero la intención es demostrar la división por cero.\nSi \\(R^2_1\\) fuera exactamente \\(1\\), el denominador sería \\(0\\), y el \\(FIV\\) sería infinito. Cuando \\(R^2_1\\) es \\(1.0\\), R devuelve ‘Inf’. Por ejemplo, si lo forzamos a 1: \\(1 / (1 - 1)\\) Esto resulta en Inf\nLa varianza \\(Var(\\beta_1|X)\\) es proporcional al \\(FIV\\). Como \\(R^2_1\\) es \\(1\\), el \\(FIV\\) tiende a infinito. Por lo tanto, la varianza de los estimadores \\(Var(\\beta_1|X\\) y \\(Var(\\beta_2|X)\\) es infinita, confirmando que la información de la muestra no permite estimar los efectos individuales."
  },
  {
    "objectID": "files/Ejercicios4Quarto.html#ejercicio-3",
    "href": "files/Ejercicios4Quarto.html#ejercicio-3",
    "title": "Ejercicios4Quarto",
    "section": "",
    "text": "set.seed(1010)\nn &lt;- 100           # Número de observaciones\nbeta0 &lt;- 2         # Verdadero intercepto\nbeta1 &lt;- 1.5       # Verdadero efecto de x1\nbeta2 &lt;- 2.0       # Verdadero efecto de x2\nsigma_u &lt;- 3       # Desviación estándar del error siendo que la varianza es 9\n\n\nx1 &lt;- runif(n, min = 0, max = 100) \nx2 &lt;- runif(n, min = 0, max = 100) \nu &lt;- rnorm(n, mean = 0, sd = sigma_u) \ny &lt;- beta0 + beta1 * x1 + beta2 * x2 + u \nsimulacion1a &lt;- data.frame(y, x1, x2)\n\n\n\n\n\nmodelo3b &lt;- lm(y ~ x1 + x2, data = simulacion1a) #estimación\nsummary(modelo3b) #x1 subestimado y x2 sobreestimado\n\n\n\n\n\nmodelo_aux_x1 &lt;- lm(x1 ~ x2, data = simulacion1a)\nR2_1 &lt;- summary(modelo_aux_x1)$r.squared\nFIV1 &lt;- 1 / (1 - R2_1)\nprint(FIV1)\n\nmodelo_aux_x2 &lt;- lm(x2 ~ x1, data = simulacion1a)\nR2_2 &lt;- summary(modelo_aux_x2)$r.squared\nFIV2 &lt;- 1 / (1 - R2_2)\nprint(FIV2)\n\nEl valor bajo y cerca de \\(1\\) no nos sorprende. En el MGD(1), \\(x_1\\) y \\(x_2\\) se generaron de forma i.i.d. (independiente e idénticamente distribuidas) y uniforme, por lo que se espera que la correlación entre ellas sea cercana a cero\nControlamos con el paquete car si lo hemos calculado bien\n\nlibrary(car)\nvif(modelo3b)\n\n\n\n\n\nbeta0 &lt;- 2\nbeta1 &lt;- 0.5\nbeta2 &lt;- 0.25\nsigma_v &lt;- 7 # Raíz de la varianza (49)\nset.seed(1010)\n\nx1 &lt;- runif(n, min = 0, max = 100) \nv &lt;- rnorm(n, mean = 0, sd = sigma_v)\nx2 &lt;- 2 * x1 + v \n\nu &lt;- rnorm(n, mean = 0, sd = sigma_u)\ny &lt;- beta0 + beta1 * x1 + beta2 * x2 + u\n\nsimulacion1b &lt;- data.frame(y, x1, x2)\n\n\nmodelo3d &lt;- lm(y ~ x1 + x2, data = simulacion1b)\nsummary(modelo3d)\n\n\nmodelo_aux_x1 &lt;- lm(x1 ~ x2, data = simulacion1b)\nR2_1 &lt;- summary(modelo_aux_x1)$r.squared\nFIV1 &lt;- 1 / (1 - R2_1)\nprint(FIV1)\n\nmodelo_aux_x2 &lt;- lm(x2 ~ x1, data = simulacion1b)\nR2_2 &lt;- summary(modelo_aux_x2)$r.squared\nFIV2 &lt;- 1 / (1 - R2_2)\nprint(FIV2)\n\nLos FIVs son muy altos debidos a los \\(R^2\\) muy altos. Hay colinearidad\n\n\n\n\nn &lt;- 4000\nset.seed(1010)\n\nx1 &lt;- runif(n, min = 0, max = 100) \nv &lt;- rnorm(n, mean = 0, sd = sigma_v)\nx2 &lt;- 2 * x1 + v \n\nu &lt;- rnorm(n, mean = 0, sd = sigma_u)\ny &lt;- beta0 + beta1 * x1 + beta2 * x2 + u\n\nsimulacion1c &lt;- data.frame(y, x1, x2)\n\n\nmodelo3e &lt;- lm(y ~ x1 + x2, data = simulacion1c)\nsummary(modelo3e)\n\n\nmodelo_aux_x1 &lt;- lm(x1 ~ x2, data = simulacion1c)\nR2_1 &lt;- summary(modelo_aux_x1)$r.squared\nFIV1 &lt;- 1 / (1 - R2_1)\nprint(FIV1)\n\nmodelo_aux_x2 &lt;- lm(x2 ~ x1, data = simulacion1c)\nR2_2 &lt;- summary(modelo_aux_x2)$r.squared\nFIV2 &lt;- 1 / (1 - R2_2)\nprint(FIV2)\n\nLos FIVs son muy altos debidos a los \\(R^2\\) muy altos. Hay colinearidad pero los estimadores son más cercanos a los verdaderos porqué ahora son consistentes debido a un \\(N\\) muy alto"
  },
  {
    "objectID": "files/Ejercicios4Quarto.html#ejercicio-4",
    "href": "files/Ejercicios4Quarto.html#ejercicio-4",
    "title": "Ejercicios4Quarto",
    "section": "",
    "text": "El resultado debe ser \\(10,000\\) estimaciones para cada parámetro.\n\n\n\n\nset.seed(1010)\nreps &lt;- 10000\nn &lt;- 40\nbeta0 &lt;- 2\nbeta1 &lt;- 1.5\nbeta2 &lt;- 2.0\nsigma_u &lt;- 3\n\n\nmontecarlo1 &lt;- matrix(NA, nrow = reps, ncol = 3, \n                      dimnames = list(NULL, c(\"b0\", \"b1\", \"b2\")))\n\nx1 &lt;- runif(n, min = 0, max = 100) # x1 ~ U(0, 100)\nx2 &lt;- runif(n, min = 0, max = 100) # x2 ~ U(0, 100)\n\nfor (r in 1:reps) {\n  u &lt;- rnorm(n, mean = 0, sd = sigma_u) \n  y &lt;- beta0 + beta1 * x1 + beta2 * x2 + u\n  \n  simulacion4b &lt;- data.frame(y, x1, x2) #base de datos con y\n  modelo4b &lt;- lm(y ~ x1 + x2, data = simulacion4b) #modelo\n  \n  montecarlo1[r, ] &lt;- coef(modelo4b) #guardamos los coeficientes en la matriz\n}\n\nhead(montecarlo1)\n\n\nmedia_mc &lt;- colMeans(montecarlo1) #calculamos el promedio de los coeficientes obtenidos\nsd_mc &lt;- apply(montecarlo1, 2, sd) #calculamos la desviación estándard de los coeficientes obtenidos\nprint(media_mc) #parámetros finales\nprint(sd_mc) #desviación estándards finales\n\n\n\n\n\nestimaciones_b1 &lt;- montecarlo1[, \"b1\"] #guardamos los beta1 estimados\n\n# Convertimos las estimaciones de b1 en un data frame para ggplot2\nb1_mc &lt;- data.frame(b1_hat = estimaciones_b1)\n\nGráfico sencillo\n\nhist(estimaciones_b1, \n     breaks = 50, \n     freq = FALSE, # Queremos la densidad en el eje vertical\n     main = \"Distribución Muestral de b1\",\n     xlab = \"Valor Estimado de b1\")\n\nGrafico complejo con ggplot2\n\n\n\n\nset.seed(1010)\nreps &lt;- 10000\nn &lt;- 40\nbeta0 &lt;- 2\nbeta1 &lt;- 0.5\nbeta2 &lt;- 0.25\nsigma_v &lt;- 7 # Raíz de la varianza (49)\n\n\nmontecarlo2 &lt;- matrix(NA, nrow = reps, ncol = 3, \n                      dimnames = list(NULL, c(\"b0\", \"b1\", \"b2\")))\n\nx1 &lt;- runif(n, min = 0, max = 100) # x1 ~ U(0, 100)\nv &lt;- rnorm(n, mean = 0, sd = sigma_v)\nx2 &lt;- 2 * x1 + v \n\nfor (r in 1:reps) {\n  u &lt;- rnorm(n, mean = 0, sd = sigma_u) \n  y &lt;- beta0 + beta1 * x1 + beta2 * x2 + u\n  \n  simulacion4d &lt;- data.frame(y, x1, x2) #base de datos con y\n  modelo4d &lt;- lm(y ~ x1 + x2, data = simulacion4d) #modelo\n  \n  montecarlo2[r, ] &lt;- coef(modelo4d) #guardamos los coeficientes en la matriz\n}\n\nmedia_mc &lt;- colMeans(montecarlo2) #calculamos el promedio de los coeficientes obtenidos\nsd_mc &lt;- apply(montecarlo2, 2, sd) #calculamos la desviación estándard de los coeficientes obtenidos\n\nprint(media_mc) #parámetros finales\nprint(sd_mc) #desviación estándards finales\n\nestimaciones_b1 &lt;- montecarlo2[, \"b1\"] #guardamos los beta1 estimados\n\nGráfico\n\n# Convertimos las estimaciones de b1 en un data frame para ggplot2\nb1_mc &lt;- data.frame(b1_hat = estimaciones_b1)\n\n# Gráfico sencillo\nhist(estimaciones_b1, \n     breaks = 50, \n     freq = FALSE, # Queremos la densidad en el eje vertical\n     main = \"Distribución Muestral de b1\",\n     xlab = \"Valor Estimado de b1\")\n\nInsesgadez: Ambas distribuciones están centradas en \\(0.5\\). La multicolinearidad NO provoca sesgo.\nVarianza: El histograma MGD 2 es más disperso y achatado que el MGD 1. Esto demuestra que la alta colinearidad infla la varianza, haciendo las estimaciones menos precisas"
  },
  {
    "objectID": "files/Ejercicios4Quarto.html#ejercicio-5",
    "href": "files/Ejercicios4Quarto.html#ejercicio-5",
    "title": "Ejercicios4Quarto",
    "section": "",
    "text": "library(readr)\nairfares &lt;- read_csv(\"airfares.csv\")\n\n\n\n\nmodelo5a &lt;- lm(fare ~ disthm + concen, data = airfares)\nlibrary(stargazer)\nstargazer(modelo5a, type = \"text\")\n\n\n\n\n\nY &lt;- as.matrix(airfares$fare) # Y vector (N x 1)\nX &lt;- as.matrix(cbind(1, airfares$disthm, airfares$concen)) # X matriz: [Intercepta=1, x1, x2] (N x K)\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y # Aplicamos directamente la fórmula: beta_hat = (X'X)^-1 * X'y\nprint(beta_hat)\n\n# Parámetros del modelo (n, K)\nn &lt;- nrow(X) # número observaciones       \nK &lt;- ncol(X) # número parámetros (incluyendo intercepta) \n\n\nY_hat &lt;- X %*% beta_hat \nU_hat &lt;- Y - Y_hat\nSCR &lt;- t(U_hat) %*% U_hat # Suma de Cuadrados de los Residuos\n\n\nSCT &lt;- t(Y - mean(Y)) %*% (Y - mean(Y))\nR2 &lt;- 1 - (SCR / SCT)\nprint(R2)\nR2_adj &lt;- 1 - (1 - R2) * (n - 1) / (n - K)\nprint(R2_adj)\n\n\ngl_model &lt;- n - K # grados de libertad (gl)\nsigma2_hat &lt;- SCR / gl_model # Estimador insesgado de la varianza del error\n\n# Cálculo de la Matriz de Varianza-Covarianza y Errores Estándar\nVar_beta_hat &lt;- as.numeric(sigma2_hat) * solve(t(X) %*% X) # Var_beta_hat = sigma2_hat * (X'X)^-1\nse_beta_hat &lt;- sqrt(diag(Var_beta_hat))\nprint(se_beta_hat)\n\n\n\n\n\\[\\begin{align*}\n\\widehat{fare} &= 62.688 + 8.635 \\cdot disthm + 0.663 \\cdot concen \\\\\n& \\quad (8.828) \\quad (0.342) \\quad \\quad \\quad \\quad (0.106) \\\\\nn &= 1149 \\\\\nR^2 &= 0.380\n\\end{align*}\\]\n\\(\\hat{Y}\\) representa el vector con los valores ajustados\n\n\n\nSignos:\n\n\\(\\beta_1\\) (disthm): El coeficiente es positivo (\\(+8.635\\)). Era esperable, ya que a mayor distancia, mayor precio del billete (ceteris paribus).\n\\(\\beta_2\\) (concen): El coeficiente es positivo (\\(+0.663\\)). También era esperable; a mayor concentración (menos competencia), mayor precio del billete (ceteris paribus).\n\nBondad del ajuste: El \\(R\\^2\\) es \\(0.380\\) (\\(38\\%\\)). Esto significa que la distancia y la concentración explican el \\(38\\%\\) de la variabilidad en las tarifas. Un \\(38\\%\\) es un ajuste moderado pero razonable. Indica que hay otros factores importantes (\\(62\\%\\)) que afectan al precio y no están en el modelo.\n\n\n\nManteniendo constante el nivel de concentración del mercado (concen), un aumento de una unidad en disthm (es decir, un aumento de \\(100\\) millas en la distancia del vuelo) está asociado con un aumento medio de \\(8.635\\) dólares en el precio del billete.\n\n\n\n\n#Estimación \nmodelo5e &lt;- lm(fare ~ disthm, data = airfares)\n\n# Comparación\nstargazer(modelo5a, modelo5e, type = \"text\",\n          column.labels = c(\"Modelo (1)\", \"Modelo (2)\"),\n          keep.stat = c(\"n\", \"rsq\", \"adj.rsq\"))\n\ni. Comparación R^2\nPor definición matemática, al añadir una variable a un modelo (pasar de Modelo(2) a Modelo(1)), la Suma de Cuadrados de los Residuos (\\(SCR\\)) solo puede disminuir o quedarse igual, nunca aumentar Dado que \\(R^2 = 1 - (SCR / SCT)\\) y la Suma de Cuadrados Totales (\\(SCT\\)) es idéntica para ambos modelos, el modelo con la \\(SCR\\) más baja (Modelo(1)) tendrá necesariamente el \\(R^2\\) más alto\nii. Comparación \\(\\bar{R}^2\\)\nEn el Modelo(1), el valor obtenido fue: \\(\\bar{R}^2 = 0.379\\) En el Modelo(2), el valor obtenido es más bajo y igual a \\(R^2\\) porqué es una regresión simple.\niii. Elección del Modelo\nEl \\(\\bar{R}^2\\) del Modelo (1) (\\(0.379\\)) es superior al \\(\\bar{R}^2\\) del Modelo(2) (\\(0.354\\)). Esto indica que la variable concen es relevante y aporta suficiente poder explicativo como para justificar su inclusión.\nComprobamos si el Modelo (2) sufre de Variable omitida utilizando la formula del modelo auxiliar y el \\(FIV\\)\nRegresión auxiliar para comprobar la correlación y el signo del sesgo:\n\nmodelo_auxiliar &lt;- lm(concen ~ disthm, data = airfares)\nsummary(modelo_auxiliar)\n\nPara analizar el sesgo el Modelo(2), necesitamos dos informaciones: Del Modelo(1) sabemos que \\(\\beta_2\\) (coeficiente de concen) es POSITIVO En el modelo auxiliar vemos que el coeficiente de disthm es NEGATIVO \\[Sesgo = (Positivo) * (Negativo) = NEGATIVO\\]\nEl estimador de disthm en el Modelo(2) sufre de un sesgo negativo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Elian Giordano",
    "section": "",
    "text": "Current Positions\n\nData Replicator\n\nJournal of Political Economy\n\nSubstitute Professor\n\nUniversitat Autònoma de Barcelona\n\nResearch Assistant\n\nUniversità degli Studi di Pavia\n\nResearch Assistant (incoming)\n\nInstitut d’Economia de Barcelona\n\n\n\n\nResearch Interests\nImpact Evaluation, Inequality Measurement, Social Mobility\n\n\nContact\neliangiordano7[at]gmail[dot]com"
  }
]